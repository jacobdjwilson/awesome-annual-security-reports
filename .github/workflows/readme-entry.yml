name: Update README with AI Generated Report Entry

on:
  workflow_run:
    workflows: ["PDF to Markdown Conversion"]
    types:
      - completed
    branches: 
      - main
      - development
  workflow_dispatch:
    inputs:
      force_update:
        description: 'Force update all markdown files'
        required: false
        default: 'false'
        type: boolean

permissions:
  contents: write
  pull-requests: write

jobs:
  update-readme:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    
    steps:
      - name: üîç Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: üêç Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: üì¶ Install Dependencies
        run: |
          pip install google-generativeai pyyaml mistune gitpython requests

      - name: üåø Detect Current Branch
        id: branch
        run: |
          if [ "${{ github.event_name }}" == "workflow_run" ]; then
            BRANCH="${{ github.event.workflow_run.head_branch }}"
          else
            BRANCH="${{ github.ref_name }}"
          fi
          echo "current_branch=$BRANCH" >> $GITHUB_OUTPUT
          echo "üìç Current branch: $BRANCH"

      - name: üì• Download Artifacts
        if: github.event_name == 'workflow_run'
        uses: actions/download-artifact@v4
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ github.event.workflow_run.id }}
          path: ./artifacts
        continue-on-error: true

      - name: üîç Process Artifacts
        id: artifacts
        run: |
          FILES_TO_PROCESS=""
          
          if [ -d "./artifacts" ]; then
            echo "üìÅ Found artifacts directory"
            find ./artifacts -name "*.txt" -o -name "*.json" | while read file; do
              echo "üìÑ Processing artifact: $file"
              cat "$file" || echo "‚ùå Failed to read $file"
            done
            
            # Extract file paths from artifacts
            if find ./artifacts -name "*.txt" -exec grep -l "Markdown Conversions" {} \; | head -1 > /dev/null; then
              FILES_TO_PROCESS=$(find ./artifacts -name "*.txt" -exec cat {} \; | grep "Markdown Conversions" | head -20)
            fi
          else
            echo "üìÅ No artifacts found - scanning directory instead"
          fi
          
          echo "files_to_process<<EOF" >> $GITHUB_OUTPUT
          echo "$FILES_TO_PROCESS" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: ü§ñ AI README Update
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          FILES_FROM_ARTIFACTS: ${{ steps.artifacts.outputs.files_to_process }}
          FORCE_UPDATE: ${{ github.event.inputs.force_update }}
        run: |
          python3 << 'EOF'
          import os
          import sys
          import json
          import yaml
          import re
          import glob
          from pathlib import Path
          from datetime import datetime
          import google.generativeai as genai
          import requests
          from typing import List, Dict, Optional, Tuple
          
          class ReadmeParser:
              def __init__(self, readme_path: str):
                  self.readme_path = readme_path
                  self.content = self._read_readme()
                  self.toc_structure = self._parse_toc()
                  
              def _read_readme(self) -> str:
                  try:
                      with open(self.readme_path, 'r', encoding='utf-8') as f:
                          return f.read()
                  except FileNotFoundError:
                      print(f"‚ùå README not found at {self.readme_path}")
                      return ""
                  
              def _parse_toc(self) -> Dict:
                  """Parse Table of Contents structure dynamically"""
                  # Correct pattern to capture content between <!-- TOC --> and <!-- /TOC -->
                  toc_pattern = r'## Contents\n<!-- TOC -->\n(.*?)\n<!-- /TOC -->'
                  toc_match = re.search(toc_pattern, self.content, re.DOTALL)
                  
                  if not toc_match:
                      print("‚ö†Ô∏è No Table of Contents found within <!-- TOC --> markers")
                      return {}
                  
                  toc_content = toc_match.group(1)
                  structure = {}
                  current_main_section = None
                  
                  lines = toc_content.split('\n')
                  
                  for line in lines:
                      if not line.strip(): # Skip empty lines
                          continue
                          
                      # Main section (e.g., - [Analysis Reports](#analysis-reports))
                      # Check for '- ' at the beginning of the line, and ensure it's not a subsection (which starts with '    - ')
                      if line.startswith('- ') and not line.startswith('    - '):
                          stripped_line = line.strip()
                          match = re.search(r'\[([^\]]+)\]', stripped_line)
                          if match:
                              section_name = match.group(1)
                              current_main_section = section_name
                              structure[current_main_section] = {'subsections': {}}
                          
                      # Subsection (e.g.,     - [Threat Intelligence](#threat-intelligence))
                      # Check for '    - ' at the beginning of the line (4 spaces indentation)
                      elif line.startswith('    - ') and current_main_section:
                          stripped_line = line.strip()
                          match = re.search(r'\[([^\]]+)\]', stripped_line)
                          if match:
                              subsection_name = match.group(1)
                              structure[current_main_section]['subsections'][subsection_name] = {}
                  
                  return structure
                  
              def find_section_content(self, heading_name: str) -> Tuple[int, int]:
                  """Find the start and end positions of a heading in the README"""
                  # Look for ## Heading Name
                  pattern = rf'## {re.escape(heading_name)}\n'
                  match = re.search(pattern, self.content)
                  if not match:
                      return -1, -1
                  
                  start = match.end()
                  
                  # Find the start of the next ## heading or end of file
                  next_header = re.search(r'\n## ', self.content[start:])
                  if next_header:
                      end = start + next_header.start()
                  else:
                      end = len(self.content)
                  
                  return start, end
          
          class ReportAnalyzer:
              def __init__(self, api_key: str):
                  self.api_key = api_key
                  self.models = ["gemini-2.5-flash", "gemini-2.0-flash", "gemini-1.5-flash"]
                  self.current_model = None
                  self._setup_ai()
                  
                  # Keyword mapping for categorization
                  self.keyword_mapping = {
                      'Identity and Access Management': ['identity', 'access', 'authentication', 'authorization', 'IAM', 'SSO', 'MFA'],
                      'Cloud Security': ['cloud', 'AWS', 'Azure', 'GCP', 'kubernetes', 'container', 'serverless'],
                      'Application Security': ['application', 'software', 'code', 'development', 'SAST', 'DAST', 'API'],
                      'Ransomware and Malware': ['ransomware', 'malware', 'threat', 'attack', 'breach', 'incident'],
                      'Compliance and Governance': ['compliance', 'governance', 'regulation', 'policy', 'audit', 'GRC'],
                      'Network Security': ['network', 'firewall', 'VPN', 'intrusion', 'IDS', 'IPS', 'DDoS'],
                      'Data Security': ['data', 'privacy', 'encryption', 'DLP', 'database', 'backup'],
                      'Endpoint Security': ['endpoint', 'device', 'mobile', 'laptop', 'workstation', 'MDM'],
                      'Security Operations': ['SOC', 'SIEM', 'monitoring', 'detection', 'response', 'incident'],
                      'Risk Management': ['risk', 'assessment', 'vulnerability', 'penetration', 'testing'],
                      'Emerging Technologies': ['AI', 'ML', 'IoT', 'blockchain', 'quantum', 'edge']
                  }
                  
              def _setup_ai(self):
                  """Setup AI with fallback models"""
                  if not self.api_key:
                      print("‚ö†Ô∏è No Gemini API key provided")
                      return
                      
                  genai.configure(api_key=self.api_key)
                  
                  for model_name in self.models:
                      try:
                          model = genai.GenerativeModel(model_name)
                          # Test the model
                          response = model.generate_content("Hello")
                          if response.text:
                              self.current_model = model
                              print(f"‚úÖ Using AI model: {model_name}")
                              break
                      except Exception as e:
                          print(f"‚ùå Failed to initialize {model_name}: {e}")
                          continue
                  
                  if not self.current_model:
                      print("‚ö†Ô∏è No AI models available")
                      
              def extract_info_from_path(self, file_path: str) -> Tuple[str, str, str]:
                  """Extract organization name, year, and report title from file path"""
                  path_parts = Path(file_path).parts
                  
                  # Extract year (should be in path)
                  year = "Unknown"
                  for part in path_parts:
                      if part.isdigit() and len(part) == 4:
                          year = part
                          break
                  
                  # Extract organization name and report title from filename
                  filename = Path(file_path).stem
                  
                  # Try to split filename into organization and title
                  # Common patterns: "OrgName-Report-Title-2024" or "OrgName_Report_Title_2024"
                  filename_clean = re.sub(r'(_|\-)?20\d{2}.*', '', filename)
                  
                  # Split by common delimiters
                  parts = re.split(r'[-_\s]+', filename_clean)
                  
                  if len(parts) >= 2:
                      # First part is usually organization
                      org_name = parts[0]
                      # Rest is report title
                      title_parts = parts[1:]
                      report_title = ' '.join(title_parts)
                  else:
                      # Fallback: use filename as org name
                      org_name = filename_clean
                      report_title = f"Security Report {year}"
                  
                  # Clean up organization name
                  org_name = org_name.replace('_', ' ').replace('-', ' ')
                  org_name = ' '.join(word.capitalize() for word in org_name.split())
                  
                  # Clean up report title
                  report_title = report_title.replace('_', ' ').replace('-', ' ')
                  report_title = ' '.join(word.capitalize() for word in report_title.split())
                  
                  return org_name, year, report_title
                  
              def analyze_content(self, content: str, org_name: str, year: str, report_title: str) -> Dict:
                  """Analyze content using AI"""
                  if not self.current_model:
                      return self._fallback_analysis(content, org_name, year, report_title)
                  
                  # Load summarization prompt
                  prompt_path = ".github/ai-prompts/markdown-summarization-prompt.md"
                  base_prompt = "Analyze this security report and provide a concise summary focusing on key findings and recommendations."
                  
                  try:
                      with open(prompt_path, 'r', encoding='utf-8') as f:
                          base_prompt = f.read()
                          print(f"‚úÖ Loaded AI prompt from {prompt_path}")
                  except FileNotFoundError:
                      print(f"‚ö†Ô∏è Prompt file not found at {prompt_path}, using default")
                  
                  # Truncate content for AI processing
                  truncated_content = content[:15000] if len(content) > 15000 else content
                  
                  try:
                      # Generate summary using the custom prompt
                      summary_prompt = f"{base_prompt}\n\nOrganization: {org_name}\nReport Title: {report_title}\nYear: {year}\n\nReport Content:\n{truncated_content}"
                      summary_response = self.current_model.generate_content(summary_prompt)
                      summary = summary_response.text if summary_response.text else "No summary generated"
                      
                      # Clean up the summary (remove extra whitespace, limit length)
                      summary = ' '.join(summary.split())
                      if len(summary) > 400:
                          # Truncate at sentence boundary
                          sentences = summary.split('. ')
                          truncated_summary = ""
                          for sentence in sentences:
                              if len(truncated_summary + sentence + '. ') <= 400:
                                  truncated_summary += sentence + '. '
                              else:
                                  break
                          summary = truncated_summary.strip()
                      
                      # Determine report type
                      type_prompt = f"Based on this content, is this an 'Analysis' report (detailed technical analysis) or 'Survey' report (industry survey/statistics)? Respond with only 'Analysis' or 'Survey'. Content preview: {truncated_content[:1000]}"
                      type_response = self.current_model.generate_content(type_prompt)
                      report_type = "Analysis" if "analysis" in type_response.text.lower() else "Survey"
                      
                      # Categorize by topic
                      category = self._categorize_content(truncated_content)
                      
                      return {
                          'organization': org_name,
                          'year': year,
                          'title': report_title,
                          'summary': summary,
                          'type': report_type,
                          'category': category,
                          'ai_processed': True
                      }
                      
                  except Exception as e:
                      print(f"‚ùå AI analysis failed: {e}")
                      return self._fallback_analysis(content, org_name, year, report_title)
                      
              def _categorize_content(self, content: str) -> str:
                  """Categorize content based on keywords"""
                  content_lower = content.lower()
                  scores = {}
                  
                  for category, keywords in self.keyword_mapping.items():
                      score = sum(content_lower.count(keyword.lower()) for keyword in keywords)
                      scores[category] = score
                  
                  # Return category with highest score, or default
                  best_category = max(scores, key=scores.get) if scores else "Security Operations"
                  return best_category if scores[best_category] > 0 else "Security Operations"
                  
              def _fallback_analysis(self, content: str, org_name: str, year: str, report_title: str) -> Dict:
                  """Fallback analysis without AI"""
                  category = self._categorize_content(content)
                  
                  # Simple summary - first few sentences
                  sentences = content.split('. ')
                  summary = '. '.join(sentences[:3]) + '.' if len(sentences) > 3 else content[:300]
                  
                  return {
                      'organization': org_name,
                      'year': year,
                      'title': report_title,
                      'summary': summary,
                      'type': "Analysis",  # Default type
                      'category': category,
                      'ai_processed': False
                  }
          
          class ReadmeUpdater:
              def __init__(self, readme_parser: ReadmeParser):
                  self.parser = readme_parser
                  
              def add_report_entry(self, analysis: Dict, file_path: str) -> bool:
                  """Add or update a report entry in the README"""
                  try:
                      # Determine the logical main section (e.g., "Analysis Reports")
                      logical_main_section = f"{analysis['type']} Reports"
                      # Determine the logical subsection (e.g., "Threat Intelligence")
                      logical_subsection = analysis['category']
                      
                      # Validate against the TOC structure
                      if logical_main_section not in self.parser.toc_structure:
                          print(f"‚ö†Ô∏è Logical main section '{logical_main_section}' not found in TOC structure.")
                          return False
                      
                      if logical_subsection not in self.parser.toc_structure[logical_main_section]['subsections']:
                          print(f"‚ö†Ô∏è Logical subsection '{logical_subsection}' not found under '{logical_main_section}' in TOC structure.")
                          # Try to find a similar subsection in the TOC structure
                          similar_subsection_in_toc = self._find_similar_section(logical_subsection, logical_main_section)
                          if similar_subsection_in_toc:
                              logical_subsection = similar_subsection_in_toc
                              print(f"üìç Using similar logical subsection from TOC: {logical_subsection}")
                          else:
                              return False
                      
                      # Now, find the actual heading in the README content to insert the entry.
                      # The actual headings for categories like "Threat Intelligence" are '## Threat Intelligence'.
                      target_heading_in_readme = logical_subsection
                      
                      start, end = self.parser.find_section_content(target_heading_in_readme)
                      if start == -1:
                          print(f"‚ùå Could not find content area for actual README heading: '## {target_heading_in_readme}'")
                          return False
                      
                      # Extract current entries
                      section_content = self.parser.content[start:end]
                      
                      # Create new entry in the specified format
                      entry_text = self._format_entry(analysis, file_path)
                      
                      # Check for existing entry (look for organization and year)
                      org_pattern = re.escape(analysis['organization'])
                      year_pattern = re.escape(analysis['year'])
                      existing_pattern = rf"- \[{org_pattern}\].*\({year_pattern}\)"
                      existing_match = re.search(existing_pattern, section_content)
                      
                      if existing_match:
                          print(f"üìù Updating existing entry for {analysis['organization']} ({analysis['year']})")
                          # Find the full line containing the existing entry
                          line_start = section_content.rfind('\n', 0, existing_match.start()) + 1
                          line_end = section_content.find('\n', existing_match.end())
                          if line_end == -1:
                              line_end = len(section_content)
                          
                          old_full_entry = section_content[line_start:line_end]
                          updated_section = section_content.replace(old_full_entry, entry_text)
                      else:
                          print(f"‚ûï Adding new entry for {analysis['organization']} ({analysis['year']})")
                          # Add new entry and sort
                          lines = section_content.split('\n')
                          entries = [line for line in lines if line.strip().startswith('- [')]
                          
                          entries.append(entry_text)
                          # Sort by organization name (extracted from first link)
                          entries.sort(key=lambda x: self._extract_org_name_for_sorting(x))
                          
                          # Reconstruct section
                          other_lines = [line for line in lines if not line.strip().startswith('- [')]
                          # Keep any introductory text, then add entries
                          intro_lines = [line for line in other_lines if line.strip()]
                          if intro_lines:
                              updated_section = '\n'.join(intro_lines) + '\n\n' + '\n'.join(entries) + '\n'
                          else:
                              updated_section = '\n'.join(entries) + '\n'
                      
                      # Update the full content
                      self.parser.content = (
                          self.parser.content[:start] + 
                          updated_section + 
                          self.parser.content[end:]
                      )
                      
                      return True
                      
                  except Exception as e:
                      print(f"‚ùå Error updating README: {e}")
                      return False
                      
              def _format_entry(self, analysis: Dict, file_path: str) -> str:
                  """Format entry according to specified template"""
                  # Use the extracted title from analysis
                  title = analysis['title']
                  
                  # Create organization website URL (fuzzy match)
                  org_name = analysis['organization']
                  # Remove common words and create a reasonable domain
                  domain_base = re.sub(r'\b(company|corp|corporation|inc|llc|ltd|group|security|cyber|tech|technologies)\b', '', org_name.lower())
                  domain_base = re.sub(r'[^a-z0-9]', '', domain_base)
                  
                  # If domain is too short, use original org name
                  if len(domain_base) < 3:
                      domain_base = re.sub(r'[^a-z0-9]', '', org_name.lower())
                  
                  org_url = f"https://{domain_base}.com"
                  
                  # Format the PDF path for the second link
                  pdf_path = file_path.replace('.md', '.pdf')
                  # Keep the original path structure for the link
                  pdf_path_encoded = pdf_path.replace(' ', '%20')
                  
                  # Create the formatted entry exactly as specified
                  entry = f"- [{org_name}]({org_url}) - [{title}]({pdf_path_encoded}) ({analysis['year']}) - {analysis['summary']}"
                  
                  return entry
                  
              def _extract_org_name_for_sorting(self, entry_line: str) -> str:
                  """Extract organization name from entry line for sorting"""
                  match = re.search(r'- \[([^\]]+)\]', entry_line)
                  return match.group(1) if match else entry_line
                      
              def _find_similar_section(self, target: str, main_section: str) -> Optional[str]:
                  """Find similar section name"""
                  available_sections = list(self.parser.toc_structure[main_section]['subsections'].keys())
                  
                  # Simple similarity matching
                  for section in available_sections:
                      if any(word in section.lower() for word in target.lower().split()):
                          return section
                  
                  return None
                  
              def save_readme(self) -> bool:
                  """Save the updated README"""
                  try:
                      with open(self.parser.readme_path, 'w', encoding='utf-8') as f:
                          f.write(self.parser.content)
                      return True
                  except Exception as e:
                      print(f"‚ùå Error saving README: {e}")
                      return False
          
          def main():
              print("üöÄ Starting AI README update process")
              
              # Initialize components
              readme_parser = ReadmeParser("README.md")
              if not readme_parser.content:
                  print("‚ùå Could not read README.md")
                  return
              
              analyzer = ReportAnalyzer(os.getenv('GEMINI_API_KEY'))
              updater = ReadmeUpdater(readme_parser)
              
              # Determine files to process
              files_to_process = []
              files_from_artifacts = os.getenv('FILES_FROM_ARTIFACTS', '').strip()
              force_update = os.getenv('FORCE_UPDATE', 'false').lower() == 'true'
              
              if files_from_artifacts:
                  print("üìÑ Using files from artifacts")
                  files_to_process = [f.strip() for f in files_from_artifacts.split('\n') if f.strip()]
              elif force_update or not files_from_artifacts:
                  print("üìÅ Scanning Markdown Conversions directory")
                  conversions_dir = Path("Markdown Conversions")
                  if conversions_dir.exists():
                      files_to_process = list(conversions_dir.glob("**/*.md"))
                      files_to_process = [str(f) for f in files_to_process]
              
              if not files_to_process:
                  print("‚ùå No files found to process")
                  return
              
              print(f"‚úç Found {len(files_to_process)} files to process")
              
              # Process each file
              processed_files = []
              summaries = []
              
              for file_path in files_to_process:
                  try:
                      print(f"\nüîç Processing: {file_path}")
                      
                      # Check if file exists
                      if not os.path.exists(file_path):
                          print(f"‚ùå File not found: {file_path}")
                          continue
                      
                      # Read file content
                      with open(file_path, 'r', encoding='utf-8') as f:
                          content = f.read()
                      
                      if not content.strip():
                          print(f"‚ö†Ô∏è Empty file: {file_path}")
                          continue
                      
                      # Extract organization and year
                      org_name, year, report_title = analyzer.extract_info_from_path(file_path)
                      
                      # Analyze content
                      analysis = analyzer.analyze_content(content, org_name, year, report_title)
                      
                      # Update README
                      if updater.add_report_entry(analysis, file_path):
                          processed_files.append(file_path)
                          summaries.append(f"‚úÖ {org_name} ({year}) - {analysis['category']}")
                          print(f"‚úÖ Successfully processed {org_name} ({year})")
                      else:
                          print(f"‚ùå Failed to update README for {org_name} ({year})")
                          
                  except Exception as e:
                      print(f"‚ùå Error processing {file_path}: {e}")
                      continue
              
              # Save updated README
              if processed_files:
                  if updater.save_readme():
                      print(f"\n‚úÖ Successfully updated README.md with {len(processed_files)} reports")
                      
                      # Create summary for PR
                      summary_text = f"Updated README.md with {len(processed_files)} security reports:\n\n"
                      summary_text += "\n".join(summaries)
                      
                      # Save summary for PR creation
                      with open("pr_summary.txt", "w") as f:
                          f.write(summary_text)
                      
                      # Create debug artifact
                      debug_info = {
                          'processed_files': processed_files,
                          'summaries': summaries,
                          'total_files_found': len(files_to_process),
                          'successful_updates': len(processed_files),
                          'ai_available': analyzer.current_model is not None
                      }
                      
                      with open("debug_info.json", "w") as f:
                          json.dump(debug_info, f, indent=2)
                      
                      print("\nüìã Processing Summary:")
                      print(f"   Total files found: {len(files_to_process)}")
                      print(f"   Successfully processed: {len(processed_files)}")
                      print(f"   AI analysis available: {analyzer.current_model is not None}")
                      
                  else:
                      print("‚ùå Failed to save README.md")
                      sys.exit(1)
              else:
                  print("‚ö†Ô∏è No files were successfully processed")
          
          if __name__ == "__main__":
              main()
          EOF

      - name: üì§ Upload Debug Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: readme-update-debug-${{ github.run_number }}
          path: |
            debug_info.json
            pr_summary.txt
          retention-days: 5

      - name: ‚¨ÜÔ∏è Commit and Push Changes
        if: hashFiles('pr_summary.txt') != ''
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Configure git
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          # Add changes
          git add README.md
          
          # Check if there are changes to commit
          if git diff --cached --quiet; then
            echo "‚ÑπÔ∏è No changes to commit"
            exit 0
          fi
          
          # Commit changes
          git commit -m "‚úç Update README with new security reports
          
          $(cat pr_summary.txt)"
          
          # Push changes to the current branch
          git push origin ${{ steps.branch.outputs.current_branch }}

      - name: üìã Summary
        if: always()
        run: |
          echo "## üéØ Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Branch**: ${{ steps.branch.outputs.current_branch }}" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "debug_info.json" ]; then
            echo "### ‚úç Processing Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "$(cat debug_info.json | jq -r '
              "**Files Found**: " + (.total_files_found | tostring) + "\n" +
              "**Successfully Processed**: " + (.successful_updates | tostring) + "\n" +
              "**AI Analysis Available**: " + (.ai_available | tostring) + "\n"
            ')" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ -f "pr_summary.txt" ]; then
              echo "### üìù Updated Reports" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              cat pr_summary.txt >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "### ‚ö†Ô∏è No Processing Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "No files were processed successfully." >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üîó Useful Links" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- [Workflow Run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
          echo "- [Repository](https://github.com/${{ github.repository }})" >> $GITHUB_STEP_SUMMARY
          if [ "${{ github.event_name }}" == "workflow_run" ]; then
            echo "- [Triggering Workflow](https://github.com/${{ github.repository }}/actions/runs/${{ github.event.workflow_run.id }})" >> $GITHUB_STEP_SUMMARY
          fi
