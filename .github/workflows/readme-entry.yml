name: Update README with AI Generated Report Entry

on:
  workflow_run:
    workflows: ["PDF to Markdown Conversion"]
    types:
      - completed
    branches: 
      - main
      - development
  workflow_dispatch:
    inputs:
      force_update:
        description: 'Force update all markdown files'
        required: false
        default: 'false'
        type: boolean

permissions:
  contents: write
  pull-requests: write

jobs:
  update-readme:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    
    steps:
      - name: üîç Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: üêç Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: üì¶ Install Dependencies
        run: |
          pip install google-generativeai pyyaml mistune gitpython requests

      - name: üåø Detect Current Branch
        id: branch
        run: |
          if [ "${{ github.event_name }}" == "workflow_run" ]; then
            BRANCH="${{ github.event.workflow_run.head_branch }}"
          else
            BRANCH="${{ github.ref_name }}"
          fi
          echo "current_branch=$BRANCH" >> $GITHUB_OUTPUT
          echo "üìç Current branch: $BRANCH"

      - name: üì• Download Artifacts
        if: github.event_name == 'workflow_run'
        uses: actions/download-artifact@v4
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ github.event.workflow_run.id }}
          path: ./artifacts
        continue-on-error: true

      - name: üîç Process Artifacts
        id: artifacts
        run: |
          FILES_TO_PROCESS=""
          
          if [ -d "./artifacts" ]; then
            echo "üìÅ Found artifacts directory"
            find ./artifacts -name "*.txt" -o -name "*.json" | while read file; do
              echo "üìÑ Processing artifact: $file"
              cat "$file" || echo "‚ùå Failed to read $file"
            done
            
            # Extract file paths from artifacts
            if find ./artifacts -name "*.txt" -exec grep -l "Markdown Conversions" {} \; | head -1 > /dev/null; then
              FILES_TO_PROCESS=$(find ./artifacts -name "*.txt" -exec cat {} \; | grep "Markdown Conversions" | head -20)
            fi
          else
            echo "üìÅ No artifacts found - scanning directory instead"
          fi
          
          echo "files_to_process<<EOF" >> $GITHUB_OUTPUT
          echo "$FILES_TO_PROCESS" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: ü§ñ AI README Update
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          FILES_FROM_ARTIFACTS: ${{ steps.artifacts.outputs.files_to_process }}
          FORCE_UPDATE: ${{ github.event.inputs.force_update }}
        run: |
          python3 << 'EOF'
          import os
          import sys
          import json
          import yaml
          import re
          import glob
          from pathlib import Path
          from datetime import datetime
          import google.generativeai as genai
          import requests
          from typing import List, Dict, Optional, Tuple
          
          class ReadmeParser:
              def __init__(self, readme_path: str):
                  self.readme_path = readme_path
                  self.content = self._read_readme()
                  self.toc_structure = self._parse_toc()
                  
              def _read_readme(self) -> str:
                  try:
                      with open(self.readme_path, 'r', encoding='utf-8') as f:
                          return f.read()
                  except FileNotFoundError:
                      print(f"‚ùå README not found at {self.readme_path}")
                      return ""
                  
              def _parse_toc(self) -> Dict:
                  """Parse Table of Contents structure dynamically"""
                  # Correct pattern to capture content between <!-- TOC --> and <!-- /TOC -->
                  toc_pattern = r'## Contents\n<!-- TOC -->\n(.*?)\n<!-- /TOC -->'
                  toc_match = re.search(toc_pattern, self.content, re.DOTALL)
                  
                  if not toc_match:
                      print("‚ö†Ô∏è No Table of Contents found within <!-- TOC --> markers")
                      return {}
                  
                  toc_content = toc_match.group(1)
                  structure = {}
                  current_main_section = None
                  
                  lines = toc_content.split('\n')
                  
                  for line in lines:
                      if not line.strip(): # Skip empty lines
                          continue
                          
                      # Main section (e.g., - [Analysis Reports](#analysis-reports))
                      # Check for '- ' at the beginning of the line, and ensure it's not a subsection (which starts with '    - ')
                      if line.startswith('- ') and not line.startswith('    - '):
                          stripped_line = line.strip()
                          match = re.search(r'\[([^\]]+)\]', stripped_line)
                          if match:
                              section_name = match.group(1)
                              current_main_section = section_name
                              structure[current_main_section] = {'subsections': {}}
                          
                      # Subsection (e.g.,     - [Threat Intelligence](#threat-intelligence))
                      # Check for '    - ' at the beginning of the line (4 spaces indentation)
                      elif line.startswith('    - ') and current_main_section:
                          stripped_line = line.strip()
                          match = re.search(r'\[([^\]]+)\]', stripped_line)
                          if match:
                              subsection_name = match.group(1)
                              structure[current_main_section]['subsections'][subsection_name] = {}
                  
                  return structure
                  
              def find_section_content(self, heading_name: str) -> Tuple[int, int]:
                  """Find the start and end positions of a heading in the README"""
                  # Look for ## Heading Name
                  pattern = rf'## {re.escape(heading_name)}\n'
                  match = re.search(pattern, self.content)
                  if not match:
                      return -1, -1
                  
                  start = match.end()
                  
                  # Find the start of the next ## heading or end of file
                  next_header = re.search(r'\n## ', self.content[start:])
                  if next_header:
                      end = start + next_header.start()
                  else:
                      end = len(self.content)
                  
                  return start, end
          
          class ReportAnalyzer:
              def __init__(self, api_key: str):
                  self.api_key = api_key
                  self.models = ["gemini-2.5-flash", "gemini-2.0-flash", "gemini-1.5-flash"]
                  self.current_model = None
                  self._setup_ai()
                  
                  # List of acronyms that should always be fully capitalized
                  self.acronyms = ["AI", "IT", "API", "ML", "IoT", "GRC", "IAM", "SSO", "MFA", "AWS", "GCP", "VPN", "IDS", "IPS", "DDoS", "DLP", "MDM", "SOC", "SIEM", "SAST", "DAST"]

                  # Keyword mapping for categorization
                  self.keyword_mapping = {
                      'Identity and Access Management': ['identity', 'access', 'authentication', 'authorization', 'IAM', 'SSO', 'MFA'],
                      'Cloud Security': ['cloud', 'AWS', 'Azure', 'GCP', 'kubernetes', 'container', 'serverless'],
                      'Application Security': ['application', 'software', 'code', 'development', 'SAST', 'DAST', 'API'],
                      'Ransomware and Malware': ['ransomware', 'malware', 'threat', 'attack', 'breach', 'incident'],
                      'Compliance and Governance': ['compliance', 'governance', 'regulation', 'policy', 'audit', 'GRC'],
                      'Network Security': ['network', 'firewall', 'VPN', 'intrusion', 'IDS', 'IPS', 'DDoS'],
                      'Data Security': ['data', 'privacy', 'encryption', 'DLP', 'database', 'backup'],
                      'Endpoint Security': ['endpoint', 'device', 'mobile', 'laptop', 'workstation', 'MDM'],
                      'Security Operations': ['SOC', 'SIEM', 'monitoring', 'detection', 'response', 'incident'],
                      'Emerging Technologies': ['AI', 'ML', 'IoT', 'blockchain', 'quantum', 'edge'],
                      # Keywords from 'Risk Management' distributed to existing categories
                      'Vulnerabilities': ['vulnerability', 'assessment'], # For Analysis Reports
                      'Penetration Testing': ['penetration', 'testing'], # For Survey Reports
                      'Industry Trends': ['risk'] # For broader risk discussions in Survey Reports
                  }
                  
              def _setup_ai(self):
                  """Setup AI with fallback models"""
                  if not self.api_key:
                      print("‚ö†Ô∏è No Gemini API key provided")
                      return
                      
                  genai.configure(api_key=self.api_key)
                  
                  for model_name in self.models:
                      try:
                          model = genai.GenerativeModel(model_name)
                          # Test the model
                          response = model.generate_content("Hello")
                          if response.text:
                              self.current_model = model
                              print(f"‚úÖ Using AI model: {model_name}")
                              break
                      except Exception as e:
                          print(f"‚ùå Failed to initialize {model_name}: {e}")
                          continue
                  
                  if not self.current_model:
                      print("‚ö†Ô∏è No AI models available")
                      
              def extract_info_from_path(self, file_path: str) -> Tuple[str, str, str]:
                  """Extract organization name, year, and report title from file path"""
                  path_parts = Path(file_path).parts
                  
                  # Extract year (should be in path)
                  year = "Unknown"
                  for part in path_parts:
                      if part.isdigit() and len(part) == 4:
                          year = part
                          break
                  
                  # Extract organization name and report title from filename
                  filename = Path(file_path).stem
                  
                  # Try to split filename into organization and title
                  # Common patterns: "OrgName-Report-Title-2024" or "OrgName_Report_Title_2024"
                  filename_clean = re.sub(r'(_|\-)?20\d{2}.*', '', filename)
                  
                  # Split by common delimiters
                  parts = re.split(r'[-_\s]+', filename_clean)
                  
                  if len(parts) >= 2:
                      # First part is usually organization
                      org_name = parts[0]
                      # Rest is report title
                      title_parts = parts[1:]
                      report_title = ' '.join(title_parts)
                  else:
                      # Fallback: use filename as org name
                      org_name = filename_clean
                      report_title = f"Security Report {year}"
                  
                  # Clean up organization name
                  org_name = org_name.replace('_', ' ').replace('-', ' ')
                  org_name = ' '.join(word.capitalize() for word in org_name.split())
                  
                  # Clean up report title and apply acronym capitalization
                  report_title = report_title.replace('_', ' ').replace('-', ' ')
                  
                  processed_title_words = []
                  for word in report_title.split():
                      if word.upper() in self.acronyms:
                          processed_title_words.append(word.upper())
                      else:
                          processed_title_words.append(word.capitalize())
                  
                  report_title = ' '.join(processed_title_words)
                  
                  return org_name, year, report_title
                  
              def analyze_content(self, content: str, org_name: str, year: str, report_title: str) -> Dict:
                  """Analyze content using AI"""
                  if not self.current_model:
                      return self._fallback_analysis(content, org_name, year, report_title)
                  
                  # Load summarization prompt
                  prompt_path = ".github/ai-prompts/markdown-summarization-prompt.md"
                  base_prompt = "Analyze this security report and provide a concise summary focusing on key findings and recommendations."
                  
                  try:
                      with open(prompt_path, 'r', encoding='utf-8') as f:
                          base_prompt = f.read()
                          print(f"‚úÖ Loaded AI prompt from {prompt_path}")
                  except FileNotFoundError:
                      print(f"‚ö†Ô∏è Prompt file not found at {prompt_path}, using default")
                  
                  # Truncate content for AI processing
                  truncated_content = content[:15000] if len(content) > 15000 else content
                  
                  try:
                      # Generate summary using the custom prompt
                      summary_prompt = f"{base_prompt}\n\nOrganization: {org_name}\nReport Title: {report_title}\nYear: {year}\n\nReport Content:\n{truncated_content}"
                      summary_response = self.current_model.generate_content(summary_prompt)
                      summary = summary_response.text if summary_response.text else "No summary generated"
                      
                      # Clean up the summary (remove extra whitespace, limit length)
                      summary = ' '.join(summary.split())
                      if len(summary) > 400:
                          # Truncate at sentence boundary
                          sentences = summary.split('. ')
                          truncated_summary = ""
                          for sentence in sentences:
                              if len(truncated_summary + sentence + '. ') <= 400:
                                  truncated_summary += sentence + '. '
                              else:
                                  break
                          summary = truncated_summary.strip()
                      
                      # Determine report type
                      type_prompt = f"Based on this content, is this an 'Analysis' report (detailed technical analysis) or 'Survey' report (industry survey/statistics)? Respond with only 'Analysis' or 'Survey'. Content preview: {truncated_content[:1000]}"
                      type_response = self.current_model.generate_content(type_prompt)
                      report_type = "Analysis" if "analysis" in type_response.text.lower() else "Survey"
                      
                      # Categorize by topic
                      category = self._categorize_content(truncated_content)
                      
                      return {
                          'organization': org_name,
                          'year': year,
                          'title': report_title,
                          'summary': summary,
                          'type': report_type,
                          'category': category,
                          'ai_processed': True
                      }
                      
                  except Exception as e:
                      print(f"‚ùå AI analysis failed: {e}")
                      return self._fallback_analysis(content, org_name, year, report_title)
                      
              def _categorize_content(self, content: str) -> str:
                  """Categorize content based on keywords"""
                  content_lower = content.lower()
                  scores = {}
                  
                  for category, keywords in self.keyword_mapping.items():
                      score = sum(content_lower.count(keyword.lower()) for keyword in keywords)
                      scores[category] = score
                  
                  # Return category with highest score, or default
                  best_category = max(scores, key=scores.get) if scores else "Security Operations"
                  return best_category if scores[best_category] > 0 else "Security Operations"
                  
              def _fallback_analysis(self, content: str, org_name: str, year: str, report_title: str) -> Dict:
                  """Fallback analysis without AI"""
                  category = self._categorize_content(content)
                  
                  # Simple summary - first few sentences
                  sentences = content.split('. ')
                  summary = '. '.join(sentences[:3]) + '.' if len(sentences) > 3 else content[:300]
                  
                  return {
                      'organization': org_name,
                      'year': year,
                      'title': report_title,
                      'summary': summary,
                      'type': "Analysis",  # Default type
                      'category': category,
                      'ai_processed': False
                  }
          
          class ReadmeUpdater:
              def __init__(self, readme_parser: ReadmeParser):
                  self.parser = readme_parser
                  
              def add_report_entry(self, analysis: Dict, file_path: str) -> Tuple[bool, str, int]:
                  """Add or update a report entry in the README"""
                  try:
                      # Determine the logical main section (e.g., "Analysis Reports")
                      logical_main_section = f"{analysis['type']} Reports"
                      # Determine the logical subsection (e.g., "Threat Intelligence")
                      logical_subsection = analysis['category']
                      
                      # Validate against the TOC structure
                      if logical_main_section not in self.parser.toc_structure:
                          print(f"‚ö†Ô∏è Logical main section '{logical_main_section}' not found in TOC structure.")
                          return False, "", -1
                      
                      if logical_subsection not in self.parser.toc_structure[logical_main_section]['subsections']:
                          print(f"‚ö†Ô∏è Logical subsection '{logical_subsection}' not found under '{logical_main_section}' in TOC structure.")
                          # Try to find a similar subsection in the TOC structure
                          similar_subsection_in_toc = self._find_similar_section(logical_subsection, logical_main_section)
                          if similar_subsection_in_toc:
                              logical_subsection = similar_subsection_in_toc
                              print(f"üìç Using similar logical subsection from TOC: {logical_subsection}")
                          else:
                              return False, "", -1
                      
                      # Now, find the actual heading in the README content to insert the entry.
                      # The actual headings for categories like "Threat Intelligence" are '## Threat Intelligence'.
                      target_heading_in_readme = logical_subsection
                      
                      start, end = self.parser.find_section_content(target_heading_in_readme)
                      if start == -1:
                          print(f"‚ùå Could not find content area for actual README heading: '## {target_heading_in_readme}'")
                          return False
                      
                      # Extract current entries
                      section_content = self.parser.content[start:end]
                      
                      # Create new entry in the specified format
                      entry_text = self._format_entry(analysis, file_path)
                      
                      # Create new entry in the specified format
                      entry_text = self._format_entry(analysis, file_path)
                      
                      # Check for existing entry by organization name AND report title
                      org_pattern = re.escape(analysis['organization'])
                      title_pattern = re.escape(analysis['title'])
                      # This regex looks for a line starting with '- [' followed by the organization name,
                      # then the report title, and then the year in parentheses, capturing the PDF URL.
                      existing_entry_pattern = rf"^- \[{org_pattern}\]\((?P<org_url>[^)]+)\) - \[{title_pattern}\]\((?P<pdf_url>[^)]+)\) \((\d{{4}})\).*$"
                      
                      existing_match = re.search(existing_entry_pattern, section_content, re.MULTILINE)
                      
                      updated_section = section_content
                      entry_replaced = False
                      existing_pdf_url = None
                      
                      if existing_match:
                          existing_full_entry = existing_match.group(0)
                          existing_year = int(existing_match.group(1))
                          existing_pdf_url = existing_match.group('pdf_url') # Capture existing PDF URL
                          new_report_year = int(analysis['year'])
                          
                          if new_report_year > existing_year:
                              print(f"üìù Updating existing entry for {analysis['organization']} - {analysis['title']} (from {existing_year} to {new_report_year})")
                              # Use re.sub for robust replacement of the entire line
                              updated_section = re.sub(existing_entry_pattern, entry_text, updated_section, 1, re.MULTILINE)
                              entry_replaced = True
                          else:
                              print(f"‚ÑπÔ∏è Existing entry for {analysis['organization']} - {analysis['title']} ({existing_year}) is newer or same year as new report ({new_report_year}). No update needed.")
                              return False, "", -1 # Indicate no update was performed
                      
                      if not entry_replaced:
                          print(f"‚ûï Adding new entry for {analysis['organization']} - {analysis['title']} ({analysis['year']})")
                          # Add new entry and sort
                          lines = updated_section.split('\n')
                          entries = [line for line in lines if line.strip().startswith('- [')]
                          
                          entries.append(entry_text)
                          # Sort by organization name and then by report title
                          entries.sort(key=lambda x: (self._extract_org_name_for_sorting(x), self._extract_report_title_for_sorting(x)))
                          
                          # Reconstruct section
                          other_lines = [line for line in lines if not line.strip().startswith('- [')]
                          # Keep any introductory text, then add entries
                          intro_lines = [line for line in other_lines if line.strip()]
                          if intro_lines:
                              updated_section = '\n'.join(intro_lines) + '\n\n' + '\n'.join(entries) + '\n'
                          else:
                              updated_section = '\n'.join(entries) + '\n'
                      
                      # Update the full content
                      self.parser.content = (
                          self.parser.content[:start] + 
                          updated_section + 
                          self.parser.content[end:]
                      )
                      
                      # Calculate line number of the inserted/updated entry
                      inserted_line_number = -1
                      updated_content_lines = self.parser.content.split('\n')
                      target_entry_text = entry_text # Use the newly formatted entry text
                      for i, line in enumerate(updated_content_lines):
                          if line.strip() == target_entry_text.strip():
                              inserted_line_number = i + 1
                              break
                      
                      return True, target_entry_text, inserted_line_number
                      
                  except Exception as e:
                      print(f"‚ùå Error updating README: {e}")
                      return False, "", -1
                      
              def _format_entry(self, analysis: Dict, file_path: str, existing_pdf_url: Optional[str] = None) -> str:
                  """Format entry according to specified template"""
                  # Use the extracted title from analysis
                  title = analysis['title']
                  
                  # Create organization website URL (fuzzy match)
                  org_name = analysis['organization']
                  # Remove common words and create a reasonable domain
                  domain_base = re.sub(r'\b(company|corp|corporation|inc|llc|ltd|group|security|cyber|tech|technologies)\b', '', org_name.lower())
                  domain_base = re.sub(r'[^a-z0-9]', '', domain_base)
                  
                  # If domain is too short, use original org name
                  if len(domain_base) < 3:
                      domain_base = re.sub(r'[^a-z0-9]', '', org_name.lower())
                  
                  org_url = f"https://{domain_base}.com"
                  
                  # Extract year and original filename from the markdown file_path
                  path_parts = Path(file_path).parts
                  year = "Unknown"
                  for part in path_parts:
                      if part.isdigit() and len(part) == 4:
                          year = part
                          break
                  
                  original_filename_stem = Path(file_path).stem
                  
                  # Construct the correct PDF path, using existing_pdf_url if available
                  if existing_pdf_url:
                      pdf_path_encoded = existing_pdf_url
                  else:
                      correct_pdf_path = Path("Annual Security Reports") / year / f"{original_filename_stem}.pdf"
                      pdf_path_encoded = str(correct_pdf_path).replace(' ', '%20')
                  
                  # Create the formatted entry exactly as specified
                  entry = f"- [{org_name}]({org_url}) - [{title}]({pdf_path_encoded}) ({analysis['year']}) - {analysis['summary']}"
                  
                  return entry
                  
              def _extract_org_name_for_sorting(self, entry_line: str) -> str:
                  """Extract organization name from entry line for sorting"""
                  match = re.search(r'- \[([^\]]+)\]', entry_line)
                  return match.group(1) if match else entry_line
              
              def _extract_report_title_for_sorting(self, entry_line: str) -> str:
                  """Extract report title from entry line for sorting"""
                  # This regex looks for the title part after the organization link and before the year
                  match = re.search(r'\]\(.*?\) - \[([^\]]+)\]', entry_line)
                  return match.group(1) if match else entry_line
                      
              def _find_similar_section(self, target: str, main_section: str) -> Optional[str]:
                  """Find similar section name"""
                  available_sections = list(self.parser.toc_structure[main_section]['subsections'].keys())
                  
                  # Simple similarity matching
                  for section in available_sections:
                      if any(word in section.lower() for word in target.lower().split()):
                          return section
                  
                  return None
                  
              def save_readme(self) -> bool:
                  """Save the updated README"""
                  try:
                      with open(self.parser.readme_path, 'w', encoding='utf-8') as f:
                          f.write(self.parser.content)
                      return True
                  except Exception as e:
                      print(f"‚ùå Error saving README: {e}")
                      return False
          
          def main():
              print("üöÄ Starting AI README update process")
              
              # Initialize components
              readme_parser = ReadmeParser("README.md")
              if not readme_parser.content:
                  print("‚ùå Could not read README.md")
                  return
              
              analyzer = ReportAnalyzer(os.getenv('GEMINI_API_KEY'))
              updater = ReadmeUpdater(readme_parser)
              
              # Determine files to process
              files_to_process = []
              files_from_artifacts = os.getenv('FILES_FROM_ARTIFACTS', '').strip()
              force_update = os.getenv('FORCE_UPDATE', 'false').lower() == 'true'
              
              if files_from_artifacts:
                  print("üìÑ Using files from artifacts")
                  files_to_process = [f.strip() for f in files_from_artifacts.split('\n') if f.strip()]
              elif force_update or not files_from_artifacts:
                  print("üìÅ Scanning Markdown Conversions directory")
                  conversions_dir = Path("Markdown Conversions")
                  if conversions_dir.exists():
                      files_to_process = list(conversions_dir.glob("**/*.md"))
                      files_to_process = [str(f) for f in files_to_process]
              
              if not files_to_process:
                  print("‚ùå No files found to process")
                  return
              
              print(f"‚úç Found {len(files_to_process)} files to process")
              
              # Process each file
              processed_files = []
              summaries = []
              
              for file_path in files_to_process:
                  try:
                      print(f"\nüîç Processing: {file_path}")
                      
                      # Check if file exists
                      if not os.path.exists(file_path):
                          print(f"‚ùå File not found: {file_path}")
                          continue
                      
                      # Read file content
                      with open(file_path, 'r', encoding='utf-8') as f:
                          content = f.read()
                      
                      if not content.strip():
                          print(f"‚ö†Ô∏è Empty file: {file_path}")
                          continue
                      
                      # Extract organization and year
                      org_name, year, report_title = analyzer.extract_info_from_path(file_path)
                      
                      # Analyze content
                      analysis = analyzer.analyze_content(content, org_name, year, report_title)
                      
                      # Update README
                      success, entry_text, line_number = updater.add_report_entry(analysis, file_path)
                      if success:
                          processed_files.append(file_path)
                          summaries.append(f"‚úÖ Added: {entry_text}\n   (Line: {line_number})")
                          print(f"‚úÖ Successfully processed {org_name} ({year})")
                      else:
                          print(f"‚ùå Failed to update README for {org_name} ({year})")
                          
                  except Exception as e:
                      print(f"‚ùå Error processing {file_path}: {e}")
                      continue
              
              # Save updated README
              if processed_files:
                  if updater.save_readme():
                      print(f"\n‚úÖ Successfully updated README.md with {len(processed_files)} reports")
                      
                      # Create summary for PR
                      summary_text = f"Updated README.md with {len(processed_files)} security reports:\n\n"
                      summary_text += "\n".join(summaries)
                      
                      # Save summary for PR creation
                      with open("pr_summary.txt", "w") as f:
                          f.write(summary_text)
                      
                      # Create debug artifact
                      debug_info = {
                          'processed_files': processed_files,
                          'summaries': summaries,
                          'total_files_found': len(files_to_process),
                          'successful_updates': len(processed_files),
                          'ai_available': analyzer.current_model is not None
                      }
                      
                      with open("debug_info.json", "w") as f:
                          json.dump(debug_info, f, indent=2)
                      
                      print("\nüìã Processing Summary:")
                      print(f"   Total files found: {len(files_to_process)}")
                      print(f"   Successfully processed: {len(processed_files)}")
                      print(f"   AI analysis available: {analyzer.current_model is not None}")
                      
                  else:
                      print("‚ùå Failed to save README.md")
                      sys.exit(1)
              else:
                  print("‚ö†Ô∏è No files were successfully processed")
          
          if __name__ == "__main__":
              main()
          EOF

      - name: üì§ Upload Debug Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: readme-update-debug-${{ github.run_number }}
          path: |
            debug_info.json
            pr_summary.txt
          retention-days: 5

      - name: ‚¨ÜÔ∏è Commit and Push Changes
        if: hashFiles('pr_summary.txt') != ''
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Configure git
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          # Add changes
          git add README.md
          
          # Check if there are changes to commit
          if git diff --cached --quiet; then
            echo "‚ÑπÔ∏è No changes to commit"
            exit 0
          fi
          
          # Commit changes
          git commit -m "‚úç Update README with new security reports
          
          $(cat pr_summary.txt)"
          
          # Push changes to the current branch
          git push origin ${{ steps.branch.outputs.current_branch }}

      - name: üìã Summary
        if: always()
        run: |
          echo "## üéØ Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Branch**: ${{ steps.branch.outputs.current_branch }}" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "debug_info.json" ]; then
            echo "### ‚úç Processing Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "$(cat debug_info.json | jq -r '
              "**Files Found**: " + (.total_files_found | tostring) + "\n" +
              "**Successfully Processed**: " + (.successful_updates | tostring) + "\n" +
              "**AI Analysis Available**: " + (.ai_available | tostring) + "\n"
            ')" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ -f "pr_summary.txt" ]; then
              echo "### üìù Updated Reports" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              cat pr_summary.txt >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "### ‚ö†Ô∏è No Processing Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "No files were processed successfully." >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üîó Useful Links" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Workflow Run - https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          if [ "${{ github.event_name }}" == "workflow_run" ]; then
            echo "Triggering Workflow - https://github.com/${{ github.repository }}/actions/runs/${{ github.event.workflow_run.id }}" >> $GITHUB_STEP_SUMMARY
          fi
