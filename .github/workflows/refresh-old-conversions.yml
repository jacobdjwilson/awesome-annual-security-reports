name: Refresh Old Conversions

on:
  schedule:
    # Run every day at 8:00 AM ET (12:00 UTC)
    - cron: '0 13 * * *'
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

jobs:
  identify-stale-files:
    runs-on: ubuntu-latest
    outputs:
      has_files: ${{ steps.check-files.outputs.has_files }}
      file_count: ${{ steps.check-files.outputs.file_count }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Find stale or missing files
        id: check-files
        run: |
          MD_ROOT="Markdown Conversions"
          PDF_ROOT="Annual Security Reports"
          LIMIT=10
          DAYS_OLD=90
          THRESHOLD=$(date -d "$DAYS_OLD days ago" +%s)
          
          echo "Scanning for missing or stale conversions..."
          > candidates.txt

          # 1. Batch find MISSING conversions (PDF exists, MD does not)
          # We use a subshell to check for MD existence for every PDF found
          find "$PDF_ROOT" -name "*.pdf" | while read -r pdf; do
            rel_path="${pdf#$PDF_ROOT/}"
            md_path="$MD_ROOT/${rel_path%.pdf}.md"
            if [ ! -f "$md_path" ]; then
              echo "0 $pdf" >> candidates.txt # Priority 0 for missing files
            fi
          done

          # 2. Batch find STALE conversions using Git (High Performance)
          # Get all MD files and their last commit timestamp in one go
          git ls-files -z "$MD_ROOT" | xargs -0 -I {} git log -1 --format="%ct {}" -- {} >> all_timestamps.txt || true
          
          while read -r timestamp file; do
            if [ "$timestamp" -lt "$THRESHOLD" ]; then
              rel_path="${file#$MD_ROOT/}"
              pdf_path="$PDF_ROOT/${rel_path%.md}.pdf"
              if [ -f "$pdf_path" ]; then
                # Store with timestamp for sorting (oldest first)
                echo "$timestamp $pdf_path" >> candidates.txt
              fi
            fi
          done < all_timestamps.txt

          # 3. Sort, Limit, and Format Output
          # Sort by timestamp (column 1), unique by file path (column 2), take top 10
          sort -k1,1n candidates.txt | cut -d' ' -f2- | head -n $LIMIT > files_to_process.txt

          if [ -s files_to_process.txt ]; then
            COUNT=$(wc -l < files_to_process.txt)
            echo "has_files=true" >> "$GITHUB_OUTPUT"
            echo "file_count=$COUNT" >> "$GITHUB_OUTPUT"
            echo "Selected $COUNT files for processing."
          else
            echo "has_files=false" >> "$GITHUB_OUTPUT"
            echo "file_count=0" >> "$GITHUB_OUTPUT"
          fi

      - name: Upload file list
        if: steps.check-files.outputs.has_files == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: files-to-process
          path: files_to_process.txt
          retention-days: 1

  process-stale-reports:
    needs: identify-stale-files
    if: needs.identify-stale-files.outputs.has_files == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y tesseract-ocr
          python -m pip install --upgrade pip
          pip install markitdown[pdf,ocr] google-generativeai google-api-python-client requests

      - name: Download file list
        uses: actions/download-artifact@v4
        with:
          name: files-to-process
          path: .

      - name: Re-run PDF Converter
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GOOGLE_SEARCH_API_KEY: ${{ secrets.GOOGLE_SEARCH_API_KEY }}
          GOOGLE_CSE_ID: ${{ secrets.GOOGLE_CSE_ID }}
        run: |
          PROMPT_PATH=".github/ai-prompts/pdf-to-markdown-prompt.md"
          python .github/scripts/pdf-converter.py files_to_process.txt "$PROMPT_PATH" "refresh-job" "${{ github.ref_name }}" --output-json conversions.json

      - name: Re-run Report Analyzer
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          if [ -f conversions.json ]; then
            python .github/scripts/report-analyzer.py conversions.json --readme-path README.md --output-json analysis.json
          fi

      - name: Update README
        id: readme_update
        run: |
          if [ -f analysis.json ]; then
            if python .github/scripts/readme-updater.py analysis.json --readme-path README.md; then
              if git diff --quiet HEAD -- README.md "Markdown Conversions/"; then
                echo "changes_detected=false" >> $GITHUB_OUTPUT
              else
                echo "changes_detected=true" >> $GITHUB_OUTPUT
              fi
            fi
          fi

      - name: Create Pull Request
        if: steps.readme_update.outputs.changes_detected == 'true'
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "♻️ Refresh stale and missing security reports"
          title: "♻️ Sync/Refresh ${{ needs.identify-stale-files.outputs.file_count }} security reports"
          body: |
            ### Automated Report Sync
            
            This workflow identified reports that were either:
            1. **Missing**: PDF exists but no Markdown version was found.
            2. **Stale**: Markdown version is older than 90 days.
          branch: refresh-reports-${{ github.run_id }}
          delete-branch: true
          add-paths: |
            README.md
            Markdown Conversions/
          labels: |
            automated
            maintenance