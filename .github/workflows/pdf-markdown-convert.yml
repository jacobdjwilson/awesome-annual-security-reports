name: PDF to Markdown Conversion

on:
  push:
    branches: [main, development]
    paths:
      - 'Annual Security Reports/**/*.pdf'
  pull_request:
    branches: [main, development]
    paths:
      - 'Annual Security Reports/**/*.pdf'
  workflow_dispatch:

jobs:
  convert-pdf-to-markdown:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install 'markitdown[pdf]' google-generativeai
      
      - name: Find changed PDF files
        id: find-pdfs
        run: |
          CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD || echo ${GITHUB_REF#refs/heads/})
          echo "Current branch: $CURRENT_BRANCH"
          
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # For pull requests
            CHANGED_FILES=$(git diff --name-only ${{ github.event.pull_request.base.sha }} ${{ github.event.pull_request.head.sha }} | grep -E "Annual Security Reports/.*\.pdf$" || true)
          elif [[ -n "${{ github.event.before }}" && "${{ github.event.before }}" != "0000000000000000000000000000000000000000" ]]; then
            # For direct pushes
            CHANGED_FILES=$(git diff --name-only ${{ github.event.before }} ${{ github.event.after }} | grep -E "Annual Security Reports/.*\.pdf$" || true)
          else
            # For manual runs or initial pushes
            CHANGED_FILES=$(find "Annual Security Reports" -name "*.pdf" 2>/dev/null || true)
          fi
          
          if [[ -z "$CHANGED_FILES" ]]; then
            echo "No PDF files found. Exiting."
            echo "pdfs_changed=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "pdfs_changed=true" >> $GITHUB_OUTPUT
          echo "pdf_files<<EOF" >> $GITHUB_OUTPUT
          echo "$CHANGED_FILES" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          echo "branch=$CURRENT_BRANCH" >> $GITHUB_OUTPUT
      
      - name: Verify prompt file exists
        id: check-prompt
        run: |
          PROMPT_PATH=".github/ai-prompts/pdf-to-markdown-prompt.md"
          if [ -f "$PROMPT_PATH" ]; then
            echo "prompt_exists=true" >> $GITHUB_OUTPUT
            echo "Found prompt file at $PROMPT_PATH"
            PROMPT_VERSION=$(git log -n 1 --pretty=format:"%s" -- "$PROMPT_PATH" | grep -oP "v\d+\.\d+(\.\d+)?" || echo "v1.0")
            echo "prompt_version=$PROMPT_VERSION" >> $GITHUB_OUTPUT
          else
            echo "prompt_exists=false" >> $GITHUB_OUTPUT
            echo "Prompt file not found at $PROMPT_PATH"
          fi
      
      - name: Create Python script
        run: |
          cat > process_pdfs.py << 'ENDPYTHONSCRIPT'
          import os
          import sys
          import google.generativeai as genai
          from pathlib import Path
          import subprocess
          from markitdown import MarkItDown
          
          # Setup Gemini API
          genai.configure(api_key=os.environ["GEMINI_API_KEY"])
          
          # Print available models for debugging
          def list_available_models():
              try:
                  print("Attempting to list available models...")
                  models = genai.list_models()
                  print("Available models:")
                  for model in models:
                      print(f"- {model.name}")
                  return models
              except Exception as e:
                  print(f"Error listing models: {str(e)}")
                  return []
          
          # List available models
          available_models = list_available_models()
          
          # Define models to try in order of preference - using currently available models
          MODELS = ["gemini-2.5-flash", "gemini-2.0-flash", "gemini-1.5-flash"]
          
          # Get the first available model
          MODEL = None
          for model in MODELS:
              try:
                  print(f"Attempting to use model: {model}")
                  test_model = genai.GenerativeModel(model)
                  # Test a simple generation to verify it works
                  test_response = test_model.generate_content("Hello")
                  MODEL = model
                  print(f"Successfully verified model: {MODEL}")
                  break
              except Exception as e:
                  print(f"Model {model} not available or failed verification: {str(e)}")
                  continue
          
          if not MODEL:
              print("No models available. Exiting.")
              sys.exit(1)
          
          def read_prompt_file(path):
              try:
                  with open(path, "r") as file:
                      return file.read()
              except Exception as e:
                  print(f"Error reading prompt file: {str(e)}")
                  sys.exit(1)
          
          def extract_text_from_pdf(pdf_path):
              try:
                  # Use markitdown to extract text from PDF
                  print(f"Extracting text from PDF: {pdf_path}")
                  md = MarkItDown(enable_plugins=False)
                  result = md.convert(str(pdf_path))
                  text_length = len(result.text_content)
                  print(f"Successfully extracted {text_length} characters from PDF")
                  return result.text_content
              except Exception as e:
                  print(f"Error extracting text from PDF: {str(e)}")
                  raise
          
          def generate_markdown_with_ai(pdf_text, prompt_text):
              full_prompt = f"{prompt_text}\n\n# Report Content Below\n\n{pdf_text}"
              
              # Set generation parameters for better output
              generation_config = {
                  "temperature": 0.2,  # Lower temperature for more predictable output
                  "max_output_tokens": 8192,
              }
              
              for model_name in MODELS:
                  try:
                      print(f"Attempting to generate markdown with {model_name}...")
                      model = genai.GenerativeModel(model_name)
                      response = model.generate_content(
                          full_prompt,
                          generation_config=generation_config
                      )
                      
                      # Check if response contains valid parts
                      if response.parts:
                          markdown_content = response.text
                          print(f"Successfully generated markdown ({len(markdown_content)} characters) with {model_name}")
                          return markdown_content, model_name
                      else:
                          print(f"Model {model_name} returned no valid parts. Finish reason: {response.candidates[0].finish_reason if response.candidates else 'N/A'}")
                          if response.candidates and response.candidates[0].finish_reason == 2: # SAFETY
                              print(f"Safety filter triggered for model {model_name}. Trying next model.")
                              continue
                          else:
                              raise Exception(f"Model {model_name} returned no valid parts or an unexpected finish reason.")
                  except Exception as e:
                      print(f"Error generating markdown with AI using {model_name}: {str(e)}")
                      continue
              
              raise Exception("All available models failed to generate markdown.")
          
          def process_pdf(pdf_path, prompt_path, prompt_version, branch):
              """Process a PDF file and convert it to markdown
              
              Args:
                  pdf_path: Path to the PDF file
                  prompt_path: Path to the prompt file
                  prompt_version: Version of the prompt
                  branch: Current branch (main or development)
                  
              Returns:
                  bool: True if successful, False otherwise
              """
              print(f"Processing: {pdf_path} (Branch: {branch})")
              
              # Read the prompt from repository
              prompt_text = read_prompt_file(prompt_path)
              print(f"Loaded prompt file ({len(prompt_text)} characters)")
              
              # Extract text from PDF using markitdown
              pdf_text = extract_text_from_pdf(pdf_path)
              
              # Generate markdown with Gemini
              markdown_content, used_model = generate_markdown_with_ai(pdf_text, prompt_text)
              
              # Determine the output path that matches the input directory structure
              # e.g., Annual Security Reports/2025/file.pdf -> Markdown Conversions/2025/file.md
              relative_path = pdf_path.relative_to(Path("Annual Security Reports"))
              output_dir = Path("Markdown Conversions") / relative_path.parent
              output_path = output_dir / f"{pdf_path.stem}.md"
              
              print(f"Writing output to: {output_path}")
              
              # Create output directory if it doesn't exist
              os.makedirs(output_dir, exist_ok=True)
              
              # Write the markdown content to file, overwriting if it already exists
              with open(output_path, "w", encoding="utf-8") as f:
                  f.write(markdown_content)
              
              # Check if file was created or updated
              if not output_path.exists():
                  print(f"Error: Failed to create {output_path}")
                  return False
                  
              print(f"Created/Updated: {output_path}")
              
              # Add the converted file path to the list of converted files
              with open(os.environ.get("CONVERTED_FILES_PATH", "converted_files.txt"), "a") as f:
                  f.write(f"{output_path}\n")
              
              # Commit the file
              try:
                  # Add the file
                  subprocess.run(["git", "config", "user.name", "GitHub Action"], check=True)
                  subprocess.run(["git", "config", "user.email", "action@github.com"], check=True)
                  subprocess.run(["git", "add", str(output_path)], check=True)
                  
                  # Check if file is modified or new
                  status_result = subprocess.run(
                      ["git", "status", "--porcelain", str(output_path)], 
                      capture_output=True, 
                      text=True
                  )
                  
                  if status_result.returncode != 0:
                      print(f"Warning: Git status check failed: {status_result.stderr}")
                      action = "Updated"
                  else:
                      status = status_result.stdout.strip()
                      action = "Updated" if status.startswith("M") else "Created"
                  
                  # Commit the file
                  commit_message = f"{action} {output_path.name} from {pdf_path.name} using AI Prompt {prompt_version} (Model {used_model}, Branch {branch})"
                  subprocess.run(["git", "commit", "-m", commit_message], check=True)
                  print(f"Committed changes: {commit_message}")
                  return True
              except Exception as e:
                  print(f"Error during git operations: {str(e)}")
                  return False
          
          def main():
              if len(sys.argv) < 4:
                  print("Usage: python process_pdfs.py <pdf_paths_file> <prompt_path> <prompt_version> <branch>")
                  return 1
              
              pdf_paths_file = sys.argv[1]
              prompt_path = sys.argv[2]
              prompt_version = sys.argv[3]
              branch = sys.argv[4] if len(sys.argv) > 4 else "unknown"
              
              # Create or clear the converted files list
              converted_files_path = os.environ.get("CONVERTED_FILES_PATH", "converted_files.txt")
              with open(converted_files_path, "w") as f:
                  f.write("")  # Just create/clear the file
              
              # Read PDF paths
              with open(pdf_paths_file, "r") as f:
                  pdf_paths = [line.strip() for line in f.readlines()]
              
              print(f"Processing {len(pdf_paths)} PDF files on branch {branch}")
              
              success_count = 0
              
              # Process each PDF
              for pdf_path_str in pdf_paths:
                  pdf_path = Path(pdf_path_str)
                  try:
                      if process_pdf(pdf_path, prompt_path, prompt_version, branch):
                          success_count += 1
                  except Exception as e:
                      print(f"Error processing {pdf_path}: {str(e)}")
              
              print(f"Successfully processed {success_count}/{len(pdf_paths)} PDFs")
              return 0 if success_count > 0 else 1
          
          if __name__ == "__main__":
              sys.exit(main())
          ENDPYTHONSCRIPT

      - name: Process PDF files
        if: steps.find-pdfs.outputs.pdfs_changed == 'true' && steps.check-prompt.outputs.prompt_exists == 'true'
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          PROMPT_VERSION: ${{ steps.check-prompt.outputs.prompt_version }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          BRANCH: ${{ steps.find-pdfs.outputs.branch }}
          CONVERTED_FILES_PATH: "converted_files.txt"
        run: |
          # Create a file with the list of PDF paths
          echo "${{ steps.find-pdfs.outputs.pdf_files }}" > pdf_paths.txt
          
          # Run the Python script
          python process_pdfs.py pdf_paths.txt ".github/ai-prompts/pdf-to-markdown-prompt.md" "$PROMPT_VERSION" "$BRANCH"
          
          # Push changes
          git push https://${GITHUB_ACTOR}:${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}.git HEAD:${GITHUB_REF}
      
      - name: Upload converted files info
        if: steps.find-pdfs.outputs.pdfs_changed == 'true' && steps.check-prompt.outputs.prompt_exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: converted_file_info
          path: converted_files.txt
          retention-days: 1
      
      - name: Error - Prompt file missing
        if: steps.find-pdfs.outputs.pdfs_changed == 'true' && steps.check-prompt.outputs.prompt_exists != 'true'
        run: |
          echo "Error: Prompt file not found at .github/ai-prompts/pdf-to-markdown-prompt.md"
          echo "Please ensure the prompt file exists in your repository."
          exit 1
