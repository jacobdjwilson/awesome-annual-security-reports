# Security Outcomes Study, Volume 2: Maximizing the Top Five Security Practices

## Table of Contents
- [(Re)Introducing the Fab Five](#reintroducing-the-fab-five)
- [Key Findings](#key-findings)
- [Strategies for Proactive Technology Refresh](#strategies-for-proactive-technology-refresh)
- [Achieving Well-Integrated Security Technologies](#achieving-well-integrated-security-technologies)
- [Developing Threat Detection and Incident Response Capabilities](#developing-threat-detection-and-incident-response-capabilities)
- [Ensuring Prompt Disaster Recovery and Resilience](#ensuring-prompt-disaster-recovery-and-resilience)
- [Conclusion and Recommendations](#conclusion-and-recommendations)
- [About Cisco Secure](#about-cisco-secure)
- [Appendix: Survey Sample Demographics](#appendix-survey-sample-demographics)

---

## (Re)Introducing the Fab Five

The 2021 Cisco Security Outcomes Study sought to measure what matters most in cybersecurity management. To that end, we examined 25 general security practices and tested how each correlates with the achievement of 11 program-level outcomes. You can view these practice-outcome correlations via an interactive visualization on the 2021 Cisco Security Outcomes Study website, or download the full report.

From the testing, we uncovered that five of the 25 practices stood out from the rest in terms of total contribution to security program success across all measured outcomes.

In the pages that follow, we focus on these “Fab Five” drivers of security program success to identify strategies for maximizing their effectiveness. The “Fab Five” are:

*   **Proactive tech refresh**: The organization has a proactive tech refresh strategy to stay up-to-date with best available IT and security technologies.
*   **Well-integrated technology**: Security technologies are well-integrated and work effectively together.
*   **Timely incident response**: Incident response capabilities enable timely and effective investigation and remediation of security events.
*   **Accurate threat detection**: Threat detection capabilities provide accurate awareness of potential security events without significant blind spots.
*   **Prompt disaster recovery**: Recovery capabilities minimize impact and ensure resiliency of business functions affected by security incidents.

The broad efficacy of these practices begs the question, “Why?” What makes them so key to unlocking success? What factors make them more or less effective? How should companies implement these practices to maximize outcomes? These are the kinds of questions we want to explore in this iteration of the Security Outcomes Study.

In the pages that follow, we focus on these “Fab Five” drivers of security program success to identify strategies for maximizing their effectiveness. We do this through an independently conducted, double-blind survey of over 5,100 IT and security professionals around the world. We dig into the data, extract salient findings, and share vetted takeaways to help unlock new heights of security achievement for your organization.

---

## Key Findings

We asked over 5,100 IT and security professionals across 27 countries about their organizations’ approaches to updating and integrating security architecture, detecting and responding to threats, and staying resilient when disaster strikes. As you might imagine, they shared a wide range of insights, struggles, strategies, and successes. We analyzed every response in multiple ways, extracting key findings like those featured below.

### Update and integrate architecture
*   Modern, well-integrated IT contributes to overall program success more than any other security practice or control.
*   Newer, cloud-based architectures are much easier to refresh regularly to keep pace with the business.
*   Organizations that source mainly from a single vendor double their chances of building an integrated tech stack.
*   Integrated security technologies are seven times more likely to achieve high levels of process automation.

### Detect and respond to cyber threats
*   SecOps programs built on strong people, processes, and technology see a 3.5X performance boost over those with weaker resources.
*   Outsourced detection and response teams are perceived to be superior, but internal teams show faster mean-time-to-respond (6 days vs. 13 days).
*   Teams that extensively use threat intelligence are twice as likely to report strong detection and response capabilities.
*   Automation more than doubles the performance of less experienced people, and makes strong teams near certain (95%) to achieve SecOps success.

### Stay resilient when disaster strikes
*   Organizations with board-level oversight of business continuity and disaster recovery are the most likely (11% above average) to report having strong programs.
*   The probability of maintaining business resilience doesn’t improve until business continuity and disaster recovery capabilities cover at least 80% of critical systems.
*   Organizations that regularly test their business continuity and disaster recovery capabilities in multiple ways are 2.5 times more likely to maintain business resiliency.
*   Organizations that make chaos engineering standard practice are twice as likely to achieve high levels of resiliency.

### About the survey
*   **Sampling**: Cisco contracted a survey research firm, YouGov, to field a fully anonymous survey in mid-2021 that utilized a stratified random sampling technique.
*   **Respondents**: 5,123 active IT, security, and privacy professionals from 27 countries responded. Sample demographics can be found in the appendix.
*   **Analysis**: The Cyentia Institute conducted an independent analysis of the survey data on behalf of Cisco, and generated all results presented in this study.

---

## Strategies for Proactive Technology Refresh

Our prior study found that a proactive approach to refreshing and maintaining best-of-breed IT and security technologies contributed more to a successful cybersecurity program than any other practice. That’s no small feat considering all 25 of the practices we tested are widely considered “best practices” in their own right. So, we were keen to dig into what makes this practice so effective in this follow-up study.

As we begin digging deeper into tech refresh strategies, let’s do a quick sniff test of the freshness of existing infrastructure. We asked respondents what proportion of their active security technologies are outdated. On average, 39% of security technologies used by organizations are considered outdated. Almost 13% of respondents claim that at least 8 out of 10 security tools they use are showing their age.

This fact alone may help explain a lot of the benefits we see from a proactive tech refresh strategy. Ostensibly, newer technologies bring advanced capabilities to bear against an ever-advancing horde of cyber threats. But there’s more to it than that, so let’s keep digging into questions we asked of the data.

On average, 39% of security technologies used by organizations are considered outdated.

### Do infrastructure traits impact refresh initiatives?

In the original study, we speculated that more modern, cloud-based architectures might be more effective because they’re easier to manage and have native security measures built in. As a step toward testing that hypothesis, we asked respondents to generally describe their tech infrastructure by choosing a set of scaled descriptors, including:
*   Cloud vs. On-prem
*   Modern vs. Outdated
*   Consolidated vs. Distributed

Do these different architectural traits contribute to the efficacy of tech refresh capabilities? Very much so, according to Figure 1. Organizations with modern, consolidated, cloud-based architectures are more than twice as likely to report strong tech refresh capabilities than those using outdated, distributed, on-prem technologies. Before waving that chart around in the next cloud migration strategy meeting, however, take note that organizations with predominantly on-prem environments still perform well above par, provided they’ve modernized IT.

Sure, being cloud-native makes it easier to unshackle your tech refresh strategy, but being outdated is the more pressing issue here. When keeping older infrastructure fresh becomes an uphill battle, you might make more headway migrating to a new architecture than continuing to retrofit the old. That’s not always possible or cost-effective with legacy or critical infrastructure, of course, but the general principle still applies.

![Figure 1: Effect of IT architecture traits on tech refresh performance. This bar chart shows that organizations with modern, consolidated, cloud-based architectures have the highest percentage (81.6%) of reporting strong tech refresh capabilities, while outdated, distributed, on-prem architectures have the lowest (35.5%).](image_description_for_figure_1)

81.6% of organizations with modern, consolidated, cloud-based architectures report strong tech refresh capabilities.

### Do frequent upgrades help security keep up with business?

According to the 2021 Security Outcomes Study, the outcome most strongly correlated with a proactive tech refresh strategy was enabling the security program to keep up with the demands and growth of the business. In fact, that was the strongest practice-outcome combination across the whole study.

We asked organizations about the frequency of their IT and security upgrades, and compared those answers to their security program’s stated ability to keep up with the business. Is there a relationship between those two variables?

Yes indeed; we found steady improvement in this key outcome as the cadence of upgrades increased. Overall, organizations that upgrade IT and security technologies quarterly are about 30% more likely to excel at keeping up with the business than those who only upgrade every few years. Sounds like a good motivational poster for stressed IT teams: Keep Current and Carry On.

![Figure 2: Effect of tech refresh frequency on the security program’s ability to keep up with business. This line graph shows that as tech refresh frequency increases (from "Less often than annually" to "Quarterly"), the percentage of organizations reporting that their security program keeps up with business increases.](image_description_for_figure_2)

1 Throughout the report we will label figures with the “Overall” value for a particular practice or outcome. This value represents what the average value is among all respondents who answered that particular set of questions. It is provided for reference, and should guide you to understand who is doing better than average, and who is not up to snuff. We are also displaying uncertainty through error bars or shaded areas on some charts. When those areas overlap the “Overall” line, it means we can’t infer that particular aspect of a security program has any effect on the outcome or practice we are examining.

### What (or who) should drive tech refresh efforts?

We’ve established that frequent upgrades contribute to enabling the business, but what — or who — should drive the process of getting those upgrades done? We asked respondents to select their organization’s primary drivers for refreshing security technologies, and their responses fell into three broad categories:
*   **Vendor-driven**: Schedule is determined by a SaaS provider or is part of a larger vendor consolidation initiative (most common driver).
*   **Proactive**: On a predetermined schedule or when new features or use cases warrant an upgrade (second most common).
*   **Reactive**: In response to an incident, when tech becomes obsolete, or to satisfy compliance requirements (least common).

These drivers are interesting in and of themselves, but what we really want to know is whether such motives correlated with a stronger approach to tech refresh. The answer is found in Figure 3, which basically says that tech refresh initiatives are more successful when vendors handle them (or are at least actively involved in making them happen). Less than half of those with a reactive approach report strong refresh capabilities, compared to almost two-thirds of those that sync with vendor refresh cycles.

![Figure 3: Effect of primary drivers for upgrades on security tech refresh performance. This bar chart shows that organizations with a "Vendor-driven" upgrade strategy have the highest percentage (65.7%) of reporting strong tech refresh capabilities, followed by "Proactive" (52.2%), and then "Reactive" (48.9%).](image_description_for_figure_3)

65.7% of organizations that sync with vendor refresh cycles report strong tech refresh capabilities.

We get it — this all sounds really suspect coming from a vendor of IT and security products. But we honestly had zero influence on this finding. The survey was conducted by an independent, reputable research firm, the respondents had no idea Cisco sponsored the survey, and the well-respected Cyentia Institute analyzed the data to derive what you see in Figure 3. And for good measure, we’ll be extra cautious in interpreting these results.

We suspect much of the improvement attributed to vendor-driven approaches ties to cloud/SaaS architectures being more friendly to frequent upgrades. We’ll also note that this may be less about vendors being great and more about escaping the internal roadblocks and political quagmires that tend to impede tech refresh schedules.

In the words of Rob Base and DJ E-Z Rock, “It takes two to make a thing go right. It takes two to make it outta sight.” Who knew they were security architects! Make your refresh strategy outta sight by harnessing the inertia of your technology solution partners to drive mission outcomes.

### Upgrade for capability or compatibility?

The prior section covered which scenarios prompt organizations to upgrade technologies, and now we’ll look at why they choose one solution over another. Figure 4 relays what respondents told us about their selection criteria. Integrating well with existing tech is the clear preference, followed by solutions that offer best-of-breed capabilities or that meet particular needs. Perhaps surprisingly, minimizing cost ranks last.

![Figure 4: Primary selection criteria when refreshing security products. This pie chart shows the percentage of organizations that prioritize different selection criteria, with "Ease of integration" being the largest slice (30.9%), followed by "Best-of-breed" (17.3%), "Point solutions" (16.6%), "Preferred vendor" (14.1%), "Baseline compliance" (12.9%), and "Minimum cost" (8.2%).](image_description_for_figure_4)

We then tested these categories against an aggregated score created for each organization based on their level of achievement across the 11 security outcomes. The absolute value of the score has no particular meaning, but it does provide a point of comparison for the different tech refresh strategies. As seen in Figure 5, prioritizing integration and capabilities both drive outcomes more than selecting products based on minimizing cost or meeting baseline compliance requirements. But an integration-led approach is the only one that significantly outperforms the average.

![Figure 5: Effect of tech selection criterion on overall security outcomes score. This bar chart shows the percent difference from the mean security outcomes score for different selection criteria. "Integration" shows a positive difference (1.6%), "Capability" shows a positive difference (0.5%), and "Minimum" shows a negative difference (-4.1%).](image_description_for_figure_5)

Note that the differences here are pretty small in terms of overall program success. And it’s likely that what we’re really seeing here is a window into the broader priorities and practices of the security program. But this does suggest that softer issues like why we choose one product over another are worth considering. And if you’re struggling to rank features when refreshing or upgrading security solutions, take this as a reasonable justification to push for compatibility and capability over minimizing cost.

### What’s the security outcomes score?

We asked respondents about their organization’s level of success across 12 different security program outcomes. The first edition of the Security Outcomes Study analyzed these in detail, and you’ll see some of them examined individually in this study, too. But we also wanted to create an aggregated score that captures each organization’s level of achievement across all 12 outcomes as a measure of how the security program is performing overall. We refer to that as the ‘security outcomes score,’ and you’ll see it referenced a few times in this report.

To get the score, we used a fancy statistics technique called “Item Response Theory.” This technique enables us to score organizations based on how they’re doing across all outcomes, while at the same time accounting for the fact that some outcomes might be harder to achieve than others. This tried-and-true technique is how standardized test scores are created. The absolute value of the score has no particular meaning, but it does provide a point of comparison among programs.

---

## Achieving Well-Integrated Security Technologies

According to our last Security Outcomes Study, well-integrated security technologies that work effectively with broader IT infrastructure contribute to the likelihood of success for all program outcomes. We asked a range of questions designed to dig deeper into the factors behind that laudable feat, starting with the intentions behind security tech integrations.

According to respondents, the most common motive for integrating security technologies is to improve the efficiency of monitoring and auditing. That resonates with us too, as we’re familiar with the pain and frustration of having to check numerous consoles or dashboards to piece together some semblance of what’s happening across the network. Easier collaboration and automation were also common drivers for integrating security technologies (more on the latter coming up). We tested these motivations against reported tech integration levels and program outcomes, but the correlation wasn’t that strong. Perhaps “what” or “how” is more important than “why” when integrating security technologies? Let’s pull on that thread a little more in the following questions.

According to respondents, the most common motive for integrating security technologies is to improve the efficiency of monitoring and auditing.

### Buy or build for well-integrated tech?

We know from the prior study that integrating security technologies drives outcomes, but what’s the best way to achieve a highly integrated tech stack? Buy it that way? Build to suit? Just let it be? Let’s see if we can find out.

We asked organizations about their typical approach to security technology integration, and Figure 6 tallies the responses. Overall, more than three-quarters of organizations would rather buy integrated solutions than build them. Of those organizations, over 40% choose technologies that come with out-of-the-box integrations into their existing infrastructure. And more than 37% take that one step further and prefer to source solutions from a single vendor so they’re natively well-integrated or part of a larger platform. Just over 20% are willing to build integrations themselves, provided the product fits their needs. Few take a laissez-faire approach.

![Figure 6: Common approaches to security tech integration among all organizations. This bar chart shows the percentage of organizations using different integration strategies, with "Buy tech with out-of-the-box integration" being the most common (40.3%), followed by "Buy integrated tech from preferred vendors" (37.4%), and "Build integrations ourselves as needed" (20.9%).](image_description_for_figure_6)

Overall, more than 3/4 of organizations would rather buy integrated solutions than build them.

Figure 7 evaluates whether any of these integration approaches makes a difference. Here we again see a theme pointing to benefits from collaborating with vendors to keep technology modern and well-integrated. As seen in the chart, sticking with a preferred vendor is over twice as likely to achieve well-integrated security technologies as a hands-off approach (~69% vs. ~31%). Furthermore, according to our research, that finding remains consistent across all organization sizes, though the benefits of using a preferred vendor are somewhat higher for small and midsize firms versus large enterprises.

And yes, we’re aware that’s another suspiciously convenient finding coming from a company with an extensive, integrated security portfolio. Sure, we’re pleased to see that this result supports Cisco’s strategy...but recall that this was a double-blind study and we didn’t manipulate that result at all.

Not surprisingly, organizations that didn’t do anything extra to integrate security technologies became a self-fulfilling prophecy. We do, however, expect that some will be surprised to learn there’s virtually no difference among those that buy products with out-of-the-box integrations and those that build integrations on their own. Just under half (~49%) of organizations using each of these approaches report strong integration levels.

![Figure 7: Effect of common integration approaches on level of security tech integration. This bar chart shows the percentage of organizations with strong tech integration based on their integration strategy. "Preferred vendor" has the highest percentage (68.8%), followed by "Build our own" (49.0%) and "Out-of-box" (48.8%).](image_description_for_figure_7)

### Cloudy, with a chance of integration

We’ve heard from many organizations wrestling with the decision whether to begin (or expand) their security tech integration efforts in the cloud or in on-prem environments. If that’s you, we have some data that might help that evaluation. The good news is that many survey respondents report good results in both on-premises and cloud environments. That said, it appears to be significantly easier to achieve strong tech integration in the cloud.

![Figure 8: Effect of cloud vs. on-premises environments on level of security tech integration. This bar chart compares the percentage of organizations with strong tech integration in cloud IT (72.0%) versus on-prem IT (47.1%).](image_description_for_figure_8)

### Does integration aid automation?

Referring back to the start of this section, automation isn’t the most common motivation for tech integration. But 44% of organizations did identify it as an incentive. Motivations aside, is there evidence that well-integrated technologies actually do enable better automation of security processes? The evidence put forward in Figure 9 points to that indeed being the case.

![Figure 9: Effect of tech integration on extent of security process automation. This stacked bar chart shows the percentage of organizations with different levels of automation (0, 1, 2, 3) based on their level of tech integration (Strong vs. Weak). Organizations with strong integration have a significantly higher percentage of processes automated compared to those with weak integration.](image_description_for_figure_9)

The two horizontal bars in Figure 9 distinguish organizations based on their level of security tech integration (strong vs. weak). The color segments represent the number of major security processes (event monitoring, incident analysis, and incident response) supported by mature automation. The proportion of organizations with no automation is more than twice as high among those with weak integration. Conversely, those with well-integrated security technologies were almost seven times more likely to achieve high levels of automation for all three of these processes (4.1% vs. 28.5%). That sounds like a compelling motivation indeed!

### Which functions should be integrated?

Next, we asked respondents about their level of integration among technologies supporting the five core functions of the NIST Cybersecurity Framework (CSF). They answered on a scale ranging from highly fragmented (siloed technologies that work mostly in isolation) to highly integrated (coordinated technologies that work as a functional unit). Then we created a model to determine the effect on the overall security outcomes score for each organization.

The results in Figure 10 are fairly consistent across the five functions. Working to defragment and integrate any of the NIST CSF functional areas corresponds to an increase in security program success (+11% to ~15%). Thus, the answer to our titular question is “all of them.” But a highly integrated ‘Identify’ function boasts the biggest boost if you’re wondering where to start.

![Figure 10: Effect of integrating NIST CSF functions on overall security outcomes score. This bar chart shows the percent difference from the mean security outcomes score for integrating different NIST CSF functions. "Identify" shows the largest positive difference (7.3%), followed by "Recover" (7.1%), "Respond" (6.8%), "Detect" (6.7%), and "Protect" (5.6%).](image_description_for_figure_10)

We can’t help but see a connection between this fact and what we learned in the previous section about monitoring, auditing, and collaboration being the strongest drivers for integrating technology. Together, they seem to advocate for the foundational importance of good visibility across the enterprise. It certainly makes sense that a fragmented approach to “developing an organizational understanding to manage cybersecurity risk to systems, people, assets, data, and capabilities” (CSF language) won’t end well. You’ll see this theme further reinforced as we roll into the Threat Detection and Incident Response section.

### On integration, identification, and information

Beyond the chart we just discussed, data throughout this study consistently points to the crucial relationship between integration, identification, and information. If you can’t identify an asset or threat, you won’t know it’s there, and therefore won’t be concerned enough to establish an informed defense until it’s too late.

Figure 11 illustrates this concept well. We compared each organization’s reported level of integration within the NIST CSF ‘Identify’ function to their ability to accurately detect threats in a timely manner. Organizations with highly integrated systems for identifying critical assets and risks boasted much stronger (+41%) threat detection capabilities. So, in a real sense, fighting fragmentation and fighting foes go hand-in-hand!

![Figure 11: Effect of integrating the NIST CSF Identify function on threat detection capabilities. This bar chart compares the percentage of organizations with strong threat detection capabilities based on their integration level of the NIST CSF 'Identify' function. Highly integrated systems show a significantly higher percentage (74.1%) compared to fragmented systems (33.1%).](image_description_for_figure_11)

Organizations with highly integrated systems for identifying critical assets and risks had +41% stronger threat detection capabilities.

---

## Developing Threat Detection and Incident Response Capabilities

This section covers two separate security practice areas that both made the Fab Five in their own right. But because threat detection and incident response (IR) often share people, processes, and technologies under the banner of security operations (SecOps), we asked a set of common questions between them. Thus, it makes sense to analyze them within the same section for this study.

Nearly all (about 92%) of organizations with strong people, process, and technology achieve advanced threat detection and response capabilities.

### Prioritize people, process, or technology?

Speaking of people, processes, and technology (aka the p-p-t triad), let’s start our investigation there. Security functions are often described as a combination of all three elements, particularly in the domain of threat detection and incident response. But is any part of this security trinity more critical than the others? You know where this is going; let’s jump into the analysis.

Starting from the bottom of Figure 12, we see that only about a quarter of programs lacking strength in all facets of the p-p-t triad express confidence in their SecOps. Gaining strength in any one area — people, process, or technology — boosts that percentage up to roughly 60% to 64%, depending on which one. Strong people appear to grant a slight edge, but the overlapping confidence intervals caution against making too much of that fact. The important takeaway is that any of these offer a good starting point for building better detection and response capabilities.

Continuing up Figure 12, doing two things well moves SecOps programs solidly above the average and improves capabilities by about 15% to 20% over those that just do one thing well. Once again, it doesn’t really matter which people, process, technology pairing you choose. You just need strength in any two. It’s nice to know that there’s some freedom of choice in tailoring your organization’s SecOps roadmap, isn’t it?

And that brings us to elite programs in Figure 12 that manage to attain the SecOps trifecta. Nearly all (about 92%) of organizations with strong people, process, and technology achieve advanced threat detection and response capabilities. That’s a 3.5X performance increase compared to SecOps programs that don’t get any of those right! So, start wherever you can make the most headway, but don’t stop until you reach the p-p-t pinnacle.

![Figure 12: Effect of strong people, process, and technology on threat detection and incident response capabilities. This bar chart shows the percentage of organizations with strong detection and response capabilities based on their strengths in people, process, and technology. Having strength in all three results in the highest percentage (91.8%).](image_description_for_figure_12)

Organizations with strong people, processes, and technology see a 3.5X performance increase for threat detection and response over those lacking strength in all of these areas.

### Do zero trust and SASE enable better SecOps?

We understand that abstract descriptors like “strong technology” make it difficult to form concrete takeaways from the findings above. That’s why we posed a couple follow-up questions about specific architectures. We asked respondents about their adoption of zero trust and secure access service edge (SASE) to better understand how those approaches affect threat detection and incident response capabilities (and therefore security program outcomes).

![Figure 13: Effect of zero trust and SASE architectures on threat detection and incident response capabilities. This bar chart compares the percentage of organizations with strong threat detection and response capabilities based on their adoption level of Zero Trust and SASE. Mature adoption levels show significantly higher percentages than limited or progress levels.](image_description_for_figure_13)

Organizations that claim to have mature implementations of zero trust or SASE are about 35% more likely to report strong SecOps than those with nascent implementations. These results corroborate the evidence we shared earlier about the many benefits modern architectures can bring to cybersecurity programs.

### Do more heads mean fewer headaches?

We know that good people are important to building strong threat detection and incident response capabilities. But is it better to focus on adding more people or adding to the skills of the people you have? Obviously, that doesn’t have to be mutually exclusive, but the question remains—do we see any evidence that quantity or quality is more important when it comes to developing successful SecOps teams?

To answer that, we first calculated a ratio of SecOps staff to overall employees for all organizations. We then compared that ratio to the reported strength of detection and response capabilities. Figure 14 depicts the outcome of those calculations, and while it doesn’t fully answer the question of quantity or quality, it does offer some takeaways.

![Figure 14: Effect of security staffing ratio on threat detection and incident response capabilities. This line graph shows the percentage of organizations with strong detection and response capabilities across different security staff ratios. The highest staffing ratios show a slightly higher percentage, but the overall trend is relatively flat across most ratios.](image_description_for_figure_14)

First among these takeaways is that security staffing ratios do correlate with better threat detection and response. Organizations with the highest ratios are just over 20% more likely to report stronger capabilities than those with the lowest. BUT—see how the dotted line marking the overall average crosses through much of the shaded confidence interval in Figure 14? That basically means that organizations not on the extreme ends of the staffing scale (the majority of them) are equally likely to report strong SecOps programs.

What does all that actually mean? Well, we can say with confidence that organizations with huge security teams are significantly more likely to achieve strong detection and response capabilities than those with skeleton crews. But headcount alone won’t make all your SecOps headaches go away or guarantee success. Furthermore, even the differences between the smallest and largest staffing ratio don’t account for the performance boost associated with having strong people resources in the previous section. Thus, we’re left to infer that quality is equally—perhaps even more—important than quantity when it comes to building strong threat detection and response teams.

Security teams continue to face a severe staffing shortage. With shrunken resources and rising threats, many cybersecurity professionals are experiencing extreme stress and burnout. What proactive measures can we take to help their well-being? In this eBook, we asked industry leaders and practitioners to share their insights and stories on managing mental health.

### SecOps staffing: Yours, mine, or ours?

So, SecOps success isn’t merely about headcount, but do staffing models affect outcomes? All things being equal, is it better to outsource, insource, or share responsibilities for threat detection and response? Let’s see how the data answers that question—but be warned—it kind of speaks out of both sides of its mouth on this one.

We asked respondents about their staffing models and then compared that with the rating of their detection and response capabilities. As seen in Figure 15, organizations with predominantly insourced or outsourced teams were much more likely (+20% to 30%, respectively) to report strong SecOps programs than those with a mixed staffing model. Since most organizations said they used some form of mixed model, we thought it would be worth looking at this from a different perspective before dooming them all to failure just because the survey (seems to) indicate this outcome.

![Figure 15: Effect of staffing models on perceived threat detection and incident response capabilities. This bar chart shows the percentage of organizations with strong detection and response capabilities based on their staffing model (Insourced, Mix, Outsourced). Insourced and Outsourced models show higher percentages than the Mix model.](image_description_for_figure_15)

Organizations with predominantly insourced or outsourced teams are 20 to 30% more likely than those with a mixed staffing model to report strong SecOps programs.

In addition to asking respondents to rate the perceived strength of detection and response capabilities, we also tried to obtain more objective metrics for comparison. One of those is Mean Time to Respond (MTTR), or the average time to remediate or contain a security incident. In our background analysis outside this report, these metrics often tend to directionally agree with the subjective assessments. But the two perspectives contradicted each other in this case, as is evident from Figure 16.

![Figure 16: Effect of staffing models on Mean Time to Respond to security incidents. This bar chart shows the Mean Time to Respond (in days) for different staffing models (Insource, Mix, Outsource). Insourced teams have the lowest MTTR (6.2 days), followed by Mix (7.5 days), and then Outsource (13.3 days).](image_description_for_figure_16)

Based on Figure 16’s side of the story, organizations with internal threat detection and response teams enjoy an MTTR that’s less than half that of outsourced models (about 6 days vs. 13 days). Those with hybrid staffing models land in the middle (about 8 days), with MTTRs that aren’t quite as quick as internal teams but much faster than their mostly outsourced counterparts.

Obviously, we have a bit of a quandary here. Which measure (perspective vs. metric) is right and, more importantly, which one should you listen to in terms of making sourcing decisions. We’re going to be intentionally dodgy here and say “both” and “neither” (hey—don’t blame us for following the data’s conflicting lead here).

Of course, remediation has many elements and dependencies to it. The organization may be dependent on a vendor to issue a patch/bug fix to fully resolve a vulnerability. That patch then needs to be lab tested in their environment before being deployed into production. Suffice it to say there are a lot of moving parts involved.

In truth, it’s hard to know for sure what’s going on here. Maybe trying to collect metrics via a survey is misleading. Maybe MTTR and capability ratings are different enough that it’s possible to have a “strong” detection and response program overall, yet slower remediation rates. Maybe those programs are slower because they’re more thorough. Maybe coordinating with outsourced staff just takes longer. Maybe there’s a sense of confidence because “we’re paying the experts to do this and they’ve got it covered.” Maybe we’re seeing a SecOps version of the Dunning-Kruger Effect. It’s probably all this and more. And because of that, we suggest using this section to spark discussions rather than make decisions.

2 We use the geometric mean in this chart as it is more representative of a “typical” value. The reported MTTR was typically less than 2-3 weeks, but occasionally respondents reported months (or years!). Using the geometric mean manages to represent “typical” better without being skewed by those extremely large values.

### Is it smart to use intelligence?

Speaking of the Dunning-Kruger Effect, that’s a perfect setup for this section. We asked respondents about the use of cyber threat intelligence in their SecOps program. Most organizations (85%) say they’re using intelligence at some level, but less than a third (31%) claim to be using it extensively. Does that intel lead to better, smarter, faster threat detection and response? Well...let’s look at Figure 17.

Curiously, most organizations that don’t use threat intelligence at all seem to think they’re doing pretty well. The old adage of “ignorance is bliss” comes to mind here, especially since dipping a toe in the intel waters apparently dispels those notions (about 84% down to 46% confidence). Organizations that make extensive use of threat intelligence are nearly twice as likely to report strong detection and response capabilities compared to those with lower usage. And in an example where capability ratings and metrics agree, those that leverage intel more heavily achieve MTTRs that are about half that of non-intel users.

![Figure 17: Effect of cyber intelligence usage on threat detection and incident response capabilities. This bar chart shows the percentage of organizations with strong detection and response capabilities based on their level of threat intelligence usage (Not at all, 2, 3, Extensively). Higher usage levels show significantly higher percentages.](image_description_for_figure_17)

Psychologist and best-selling author Daniel Kahneman once said, “We’re blind to our own blindness. We have very little idea of how little we know.” Figure 17 suggests that once organizations know a little bit about the threats arrayed against them, they realize there’s a lot they don’t know. More extensive use of threat intelligence begins building back that confidence—except now it’s not so blind.

Organizations that make extensive use of threat intelligence are nearly 2X as likely to report strong detection and response capabilities.

### Is automation a substitute for people?

After reading this title, you might have assumed it was a rhetorical question. Not so fast. At the risk of drawing the ire of the entire security community, we’re going to go out on a (data) limb here to suggest that automation can, in fact, replace people. BUT keep reading before you decide to delete this report and add us to your blocked