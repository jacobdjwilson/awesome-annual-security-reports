# A Director’s Guide to AI Governance

## Table of Contents
- [Foreword](#foreword)
- [How to use this guide](#how-to-use-this-guide)
- [Resource purpose, audience & structure](#resource-purpose-audience-structure)
- [Executive summary](#executive-summary)
- [Section 1: AI and the governance imperative](#section-1-ai-and-the-governance-imperative)
  - [1.1 What is AI?](#what-is-ai)
  - [1.1.1 How is AI different from other technology?](#how-is-ai-different-from-other-technology)
  - [1.1.2 Different types of AI](#different-types-of-ai)
  - [1.2 AI and directors’ obligations](#ai-and-directors-obligations)
  - [1.3 AI and governance implications](#ai-and-governance-implications)
  - [1.4 Traditional IT governance may not be fit-for-purpose for AI](#traditional-it-governance-may-not-be-fit-for-purpose-for-ai)
  - [1.5 Aligning AI use to organisational strategy](#aligning-ai-use-to-organisational-strategy)
  - [1.6 AI-specific risk management](#ai-specific-risk-management)
- [Section 2: Practical steps for directors](#section-2-practical-steps-for-directors)
  - [2.1 Roles & responsibilities](#roles-responsibilities)
  - [2.2 Governance structures](#governance-structures)
  - [2.3 People, skills & culture](#people-skills-culture)
  - [2.4 Principles, policies & strategy](#principles-policies-strategy)
  - [2.5 Practices, processes & controls](#practices-processes-controls)
  - [2.6 Supporting infrastructure](#supporting-infrastructure)
  - [2.7 Stakeholder engagement & impact assessment](#stakeholder-engagement-impact-assessment)
  - [2.8 Monitoring, reporting & evaluation](#monitoring-reporting-evaluation)
- [Conclusion](#conclusion)
- [Appendix – Additional resources](#appendix-additional-resources)
- [Acknowledgements](#acknowledgements)
- [About AICD](#about-aicd)
- [About HTI](#about-hti)
- [Disclaimer](#disclaimer)
- [Copyright](#copyright)

---

![Human Technology Institute logo and text "A Director's Guide to AI Governance"](Image of the Human Technology Institute logo and the title of the guide.)

## Foreword
[GO TO CONTENTS](#table-of-contents)

While forms of Artificial Intelligence (AI) have been used for many years, the major development in Generative AI capabilities over recent times has prompted widespread discussion of its role in the economy and broader society.

AI, with its sophisticated pattern recognition capabilities pulled from vast datasets, has the potential to offer significant productivity and economic gains. However, alongside these benefits lie potential risks from AI system failures and/or abuse, including misuse of personal data, algorithmic discrimination and poorly controlled automated decision-making.

As stewards of organisational strategy and risk management, directors should seek to seize the opportunities and mitigate the risks of AI, with its ethical use in the interests of customers being paramount. This requires a robust governance framework that can adapt to the unique characteristics of AI systems.

Currently, research suggests that there is generally limited board oversight of AI use, with AI application often subject to inadequate controls and risk oversight. In many cases, directors and senior executives are unaware of where within the organisation’s value chain AI is being used, and how. If left unaddressed, this risks significant lost opportunities and commercial, reputational and regulatory damage, with regulators and policymakers increasingly focused on regulating AI harms.

In January 2024, we saw the Australian Government commit to a range of initiatives to support the uptake of safe and responsible AI. These include consideration of the introduction of mandatory guardrails for AI deployment in high-risk settings, consideration of labelling and watermarking of AI in high-risk settings, and clarifying and strengthening existing laws to address AI harms.

Internationally, we are seeing jurisdictions attempt to walk the policy tightrope between regulating high-risk AI uses to avoid the most significant AI harms, and ensuring innovation continues to flourish by tapping into this transformational technology.

To assist boards navigate the ethical and informed use of AI, the Australian Institute of Company Directors (AICD) has partnered with the Human Technology Institute (HTI) at the University of Technology Sydney (UTS) to provide a suite of director resources.

This includes:
- ‘A Director's Guide to AI Governance’, which provides practical guidance for boards’ using, or wishing to deploy AI within their organisations;
- ‘A Director's Introduction to AI’, which lays the foundation for knowledge of AI concepts;
- A Concise Snapshot of the ‘Eight elements of safe and responsible AI governance’; and
- a separate SME and NFP governance checklist which recognises the significance of small and medium-sized enterprises to the Australian economy and the specific needs of this sector.

We hope that by applying the ‘eight elements of safe and responsible AI governance’, directors can guide their organisations to deploy AI systems safely and responsibly for maximum strategic and competitive advantage.

**Mark Rigotti MAICD**  
CEO and Managing Director  
Australian Institute of Company Directors

**Professor Nicholas Davis MAICD**  
Co-Director  
Human Technology Institute  
University of Technology, Sydney

## How to use this guide
[GO TO CONTENTS](#table-of-contents)

Having considered all the boards on which you serve, select what applies to you:

**What we suggest you read**

- I know about ChatGPT, but I don’t know any other types of AI
- I am not clear how AI is different to other technologies
- I am unsure about the key legal obligations applying to AI use
- I am not clear about the key risks or opportunities arising from AI
- I do not know the underlying principles of safe and responsible AI
  - **A Director's Introduction to AI**

- I understand the difference between General AI and Narrow AI
- I understand how AI is different to other technologies, but am unclear how this impacts governance
- I am unsure about where AI is used within my organisation
- I am unsure about what questions to ask management about the governance and use of AI and how to assess the quality of management’s responses
  - **A Director's Guide to AI Governance**

- I am a director of a SME or NFP and do not know how to implement AI governance
  - **AI Governance Checklist for SME and NFP Directors**

## Resource purpose, audience & structure
[GO TO CONTENTS](#table-of-contents)

The purpose of this resource is to provide practical guidance for boards and directors of organisations that are using or planning to use AI systems (as distinct from developers and distributors of AI systems).

The primary audience of this resource are directors of ASX300 entities who are using, or considering deploying AI.

However, recognising the significance of small and medium-sized enterprises to the Australian economy and the specific needs of this sector, we provide an AI Governance Checklist for SME and NFP Directors.

AI technology as well as AI policy and regulation is dynamic and constantly developing. This resource is not intended to ‘cover the field’, but to provide a suggested framework for board oversight of AI use.

The resource is structured into two sections:

- Section 1 highlights a set of cross-cutting insights and implications for AI governance for directors.
- Section 2 sets out eight elements of effective, safe and responsible AI governance. It also provides key questions for directors and management responses to watch out for, and provides some case studies.

As part of this Guide you can also find a separate Concise Snapshot of the ‘Eight elements of safe and responsible AI governance’.

## Executive summary
[GO TO CONTENTS](#table-of-contents)

### ROLES & RESPONSIBILITIES
- Identify the management and board individual/body accountable for AI decision-making.
- Identify those involved in, and responsible for, AI system procurement, development and use.
- Consider whether decision-making processes applied by key accountable persons incorporate consideration of AI risk and opportunity.

### GOVERNANCE STRUCTURES
- Determine which existing or new board and management governance structure would most appropriately support AI oversight.
- Review board and management committee charters to determine whether and how they incorporate AI issues.
- Consider how external experts can be leveraged within existing governance structures.
- Consider the nature and frequency of management reporting to the board/ relevant board committee.

### PEOPLE, SKILLS & CULTURE
- Verify that management have assessed the organisation’s AI skills, capabilities and training needs, and implement upskilling programs (including at the director-level).
- Discuss the potential for AI to impact the workforce and workforce planning.
- Consider how AI governance structures can incorporate a diversity of perspectives, including expert views, to aid diversity of thought and avoid ‘group think’.

### PRINCIPLES, POLICIES & STRATEGY
- Require that AI is considered and, where appropriate, embedded, within the organisation’s strategy. AI use should have a clear business value – ‘AI for AI’s sake’ should be avoided.
- Engage with management to discuss how safe and responsible AI principles have been incorporated into relevant policies (such as AI/IT use, privacy, confidentiality and cyber security).
- Recognise that principles and policies need to be proactively implemented and enforced across the supply chain.

### PRACTICES, PROCESSES & CONTROLS
- Work with management to understand what controls are in place for AI use (e.g. risk appetite statement and risk management framework).
- Confirm with management that there are processes in place to assess supplier and vendor risk.
- Monitor and regularly review the effectiveness of controls.

### SUPPORTING INFRASTRUCTURE
- Confirm that you are broadly aware of where, within the organisation, AI is currently being used. Management can provide this information through an AI inventory.
- Confirm with management that management is aware of, and has a robust data governance framework in place to manage data collected and stored by the organisation to train AI systems.
- Focus on increasing transparency to end users about how the organisation's AI systems use data.

### STAKEHOLDER ENGAGEMENT & IMPACT ASSESSMENT
- Identify and engage with stakeholders to understand AI’s impact and stakeholder expectations of AI use and governance.
- Confirm with management that AI system design and assessment processes incorporate accessibility and inclusion practices.
- Consider whether AI-generated results/ outcomes are explained to stakeholders and whether an appeal process is available.

### MONITORING, REPORTING & EVALUATION
- Confirm that a risk-based monitoring and reporting system for mission-critical and high-risk AI systems is in place.
- Develop and implement a monitoring and reporting framework with metrics and outcomes to track and measure progress.
- Consider seeking internal and external assurance.

## Section 1: AI and the governance imperative
[GO TO CONTENTS](#table-of-contents)

### 1.1 WHAT IS AI?
[GO TO CONTENTS](#table-of-contents)

**KEY POINTS:**

- The unique characteristics of AI systems (complex pattern recognition based on large and diverse datasets) mean that traditional governance approaches may not be appropriate.
- Directors should be aware of AI's unique risks and opportunities and how these require adaptations to existing governance approaches.
- Effective AI governance should be human-centred, cross-functional, adaptive and iterative.
- Directors should align investment in AI with organisational values and embed it within broader business strategy. ‘AI for AI’s sake’ should be avoided.

The definition of AI adopted by the International Organisation for Standardization and the International Electrotechnical Commission ISO/IEC 22989 is:

> An engineered system that generates outputs such as content, forecasts, recommendations or decisions for a given set of human-defined objectives.

### 1.1.1 How is AI different from other technology?
[GO TO CONTENTS](#table-of-contents)

AI is a special form of digital software that is particularly good at predicting outputs, optimising, classifying, inferring missing data, and generating new data.

AI systems can often outperform non-AI systems, and as a result offer significant productivity, efficiency and customer experience benefits.

AI is also more versatile and scalable than traditional software because it can be replicated and adapted to new contexts at a relatively low cost. As a result of these advantages, AI is increasingly being deployed across organisational teams and functions.

However, the differences between traditional software systems and AI systems also impact governance approaches.

Traditional software systems are built from explicit rules coded by developers, such that their behaviour is inherently more predictable and understandable (even if the software itself is complex).

By contrast, AI systems are often created by defining an objective and using historical data to create an AI model that may rely on billions of inferred connections between data points to achieve its objective. This process means that it can be extremely challenging to replicate, explain or test an AI system’s output.

**BOX 1: The role of data in AI systems**

Data is the foundation of AI systems. Data, including personal information, is collected and used to train AI systems. It is both an input and an output of a deployed AI system.

The selection of data, particularly its quality, quantity, and representativeness, will significantly affect the performance of AI systems.

Through the ongoing collection of data and feedback loops, the accuracy and efficiency of AI systems should improve over time.

### 1.1.2 Different types of AI
[GO TO CONTENTS](#table-of-contents)

Box 2 provides a non-exhaustive list of systems that meet the definition of AI above.

**BOX 2: What kinds of systems are usefully defined as AI?**

- **Machine learning**: a broad set of models that have been trained on pre-existing data to produce useful outputs on new data.
- **Expert systems**: systems that use a knowledge base, inference engine and logic to mimic how humans make decisions.
- **Natural language systems**: models that can understand and use natural language and speech for tasks such as summarisation, translation, or content moderation.
- **Facial recognition technologies**: systems that verify a person, identify someone, or analyse personal characteristics using facial data drawn from photos or video.
- **Recommender systems**: systems that suggest products, services or information to a user based on user preferences, characteristics, or behaviour.
- **Automated decision-making systems**: systems that use data to classify, analyse and make decisions that affect people with little or no human intervention.
- **Robotic process automation**: systems that imitate human actions to automate routine tasks through existing digital interfaces.
- **Virtual agents and chatbots**: digital systems that engage with customers or employees via text or speech.
- **Generative AI**: systems that produce code, text, music, or images based on text or other inputs.
- **AI-powered robotics**: physical systems that use computer vision and machine learning models to move and execute tasks in dynamic environments.

General AI (or General Purpose AI) and Narrow AI are two sub-categories of AI (see Table 1).

**TABLE 1: Key differences between General AI and Narrow AI**

| Type of AI system | Description[^1] | Examples |
| :---------------- | :---------------- | :------- |
| **General AI (or General Purpose AI)** | An AI system that can be used for a broad range of tasks, both intended and unintended by developers. This includes Generative AI. | Text generation (e.g. GPT-4, Gemini), image generation (e.g. DALL.E, Midjourney), programming code generation (i.e. Codex). |
| **Narrow AI** | An AI system trained to deliver outputs for specialised, constrained tasks and uses to address a specific problem. | Search engines (e.g. Google, Bing), facial recognition (e.g. Apple Face ID), recommender systems (e.g. Amazon, Spotify, Netflix). |

[^1]: ISO, 2022. ISO-IEC-22989 Artificial intelligence concepts and terminology.

For more insight on What AI is and its relevance for directors, see Chapter 1 of *A Director’s Introduction to AI*.

### 1.2 AI and directors’ obligations
[GO TO CONTENTS](#table-of-contents)

While stand-alone AI regulation has not yet been introduced in Australia, a range of existing laws already apply to the use of AI systems – see Figure 1. Some laws place obligations on the organisation, while others apply to directors and officers individually. The Australian Government has also foreshadowed further reform of these laws to apply more directly to AI use.

For more detail on existing legal obligations, as well as Australian and international regulatory developments, see Chapter 3 of *A Director’s Introduction to AI*.

**FIGURE 1: Existing legal obligations when using AI**
![Diagram showing existing legal obligations when using AI, including Directors' duties (Good faith, best interest and proper purpose; Due care and diligence) and The legal and regulatory environment (Privacy, IP and Data Use; Consumer Protection; Duty of Care/Diligence; Work Health and Safety).](Diagram illustrating the legal and regulatory environment for AI use, covering directors' duties and various legal areas.)

In line with their directors’ duties, directors are responsible for the oversight of the organisation’s strategy and risk management processes. This includes managing AI risks and opportunities.

AI risks include both AI system failures and malicious, misleading, reckless or inappropriate AI use (see the summary at Table 2). These risks can create and amplify a range of commercial, reputational and regulatory risks to organisations (see Figure 2).

**TABLE 2: Key sources of AI risk for organisations**

| Key sources of AI risk | Examples |
| :------------------- | :------- |
| **AI system failures** – where systems create harm because they fail to perform as intended | - Poor system performance<br>- Biased system performance<br>- System fragility or unreliability<br>- Security failures or vulnerabilities |
| **Malicious, misleading, reckless, or inappropriate use** – where systems are deliberately used (whether by the organisation or external parties) in a way which creates or amplifies risk of harm | - Misleading advice<br>- Misinformation at scale<br>- Unfair or extractive use<br>- Opacity and lack of interpretability<br>- Weaponisation<br>- AI-powered cyber attacks<br>- Fraudulent and unlawful use e.g. scams<br>- Financial market manipulation<br>- Excessive deployment<br>- Deployment on vulnerable individuals |

**FIGURE 2: Risks to organisations from AI use**
![Diagram showing Amplified risks to organisations: Commercial (Commercial losses due to poor or biased AI system performance; adversarial attacks), Reputational (Damage to reputation and loss of trust due to harmful or unlawful treatment of consumers, employees or citizens), Regulatory (Breach of legal obligations that may result in fines, restrictions and require management focus).](Diagram illustrating the amplified commercial, reputational, and regulatory risks organizations face from AI use.)

On the other hand, a lack of investment in AI capabilities also leaves organisations vulnerable to a range of other risks, such as a lack of competitiveness, higher costs, lack of new product and service delivery, poorer consumer service, as well as talent acquisition and retention challenges.

The risks of action and inaction must be carefully weighed by directors alongside the organisational strategy and the risk appetite of the organisation.

For more details on AI risks and opportunities, see Chapter 2 of *A Director’s Introduction to AI*.

### 1.3 AI and governance implications
[GO TO CONTENTS](#table-of-contents)

Both the deliberate and ‘shadow’ AI use (see Box 3) throughout an organisation and its supply chains present directors with complex governance challenges.

**KEY QUESTIONS FOR DIRECTORS**

- How can we support experimentation and innovation with AI within the risk tolerance of the organisation?
- How is AI being used to support the delivery of the organisational strategy and related business goals?

**BOX 3: What is ‘shadow’ AI use?**

Shadow AI refers to employees’ unauthorised use of AI applications for work-related purposes.

The recent availability – and relatively low cost – of capable cloud-based large language models such as ChatGPT means that a significant percentage of employees and contractors are leveraging Generative AI systems for their work without the explicit knowledge, permission or oversight of management.

A 2023 Information Audit and Control Association (ISACA) poll of IT governance professionals across Australia and New Zealand found widespread employee use of Generative AI (63 per cent of respondents), despite only 36 per cent of organisations expressly permitting its use. Just 11 per cent of respondents said that their organisation has a comprehensive policy for Generative AI use.

The phenomenon of shadow AI poses a range of amplified risks to organisations and their stakeholders, including breaches of privacy and confidentiality.

### 1.4 Traditional IT governance may not be fit-for-purpose for AI
[GO TO CONTENTS](#table-of-contents)

Some managers and directors may be tempted to place the oversight of AI systems within existing IT governance systems. However, HTI’s research strongly suggests that existing IT risk management frameworks and systems are largely unsuited for AI governance.

This is because traditional IT governance focuses on point-in-time risk assessments of officially sanctioned systems, largely based on vendor assurances.

Such an approach has limitations in governing AI systems because:

**SPEED AND RATE OF CHANGE**

How organisations use AI is not a ‘tomorrow’ challenge – it is a ‘now’ challenge that involves rapidly advancing technologies.

**OPACITY**

Opacity in the sense of (1) the challenge of testing, validating, explaining and reproducing AI system outputs; and (2) difficulty identifying AI use within an organisation and its value chain.

**DIVERSITY OF USE CASES**

AI use crosses organisational barriers and reporting lines. Its use ranges from being used by frontline workers, being embedded within the core of the organisation’s strategy and risk management approaches, and being embedded within existing systems (e.g. software updates) and supply chains. This decentralisation across the porous boundaries of the organisation makes AI use difficult to control.

**AN UNCERTAIN POLICY, REGULATORY AND TECHNOLOGY ENVIRONMENT**

These uncertainties are driven by local and international regulatory change, technology change, and a shifting threat environment.

Organisations may be tempted to place the entirety of AI system oversight within a risk and compliance function. However, this can mean the significant opportunities of AI systems are not appropriately recognised.

In meeting these challenges, directors need to engage with management to implement an iterative, integrated, flexible and adaptive governance approach which is:

**HUMAN-CENTRED**

This refers to governance mechanisms meaningfully and transparently tracking and reporting how AI systems are impacting key stakeholders (consumers, employees, suppliers, contracting parties, etc).

**CROSS-FUNCTIONAL**

AI governance cannot be achieved through the establishment of separate, disconnected roles or policies and procedures. AI governance needs to span various departments and roles, including those responsible for privacy, IT, legal, product design and development, procurement, HR, risk and strategy. It must also be led at a senior level within the organisation.

**INTEGRATED**

Effective frameworks will integrate all eight elements set out in Section 2, rather than cherry-pick one or two.

**ITERATIVE AND ADAPTIVE**

Given the speed of technological transformation, organisations should not rely on a ‘set and forget’ approach to AI governance. Governance systems and processes should be subject to regular review to monitor whether targets and outcomes are being achieved.

### 1.5 Aligning AI use to organisational strategy
[GO TO CONTENTS](#table-of-contents)

The use of AI by organisations should be aligned to the broader organisational strategy. How AI is being used to achieve strategic objectives is core to the work of the board.

The organisational strategy should be regularly reviewed to clarify and adjust the role of AI and emerging technologies.

**KEY QUESTIONS FOR DIRECTORS TO ASK**

- How is AI currently being used to deliver business goals?
- What investments are we making in relation to the development and use of AI systems?
- How can we leverage AI in a responsible way to achieve our organisational strategy?
- What sorts of problems and challenges can or should AI systems be used to solve?
- Under what circumstances would we conclude that AI is not the right tool for the job?
- What is our overall assessment of the evolving balance between the risks and benefits of AI systems to drive business value?

### 1.6 AI-specific risk management
[GO TO CONTENTS](#table-of-contents)

As detailed in Chapter 3 of *A Director’s Introduction to AI*, directors have legal duties to effectively oversee the management and mitigation of organisational risks.

AI system use – or failure to make use of AI systems when appropriate – by organisations can pose a range of risks (see Chapter 2 of *A Director’s Introduction to AI*) that need to be carefully managed.

**SUGGESTED DIRECTOR STEPS**

- Understand current AI use, which can include the issue of an AI inventory (see Box 4).
- Review the organisational risk framework to test its application to AI use, noting increased scrutiny by stakeholders over how AI risks are being managed (see Chapter 3 of *A Director’s Introduction to AI*).
- Define and review the organisation’s risk appetite and risk statement to cover AI use.
- Align risk management approaches with existing sectoral risk management obligations (such as that required for financial services organisations under section 912A Corporations Act 2001 (Cth)).

## Section 2: Practical steps for directors
[GO TO CONTENTS](#table-of-contents)

### 2.1 Roles & responsibilities
[GO TO CONTENTS](#table-of-contents)

This chapter focuses on the practical steps that directors can take in the boardroom and in conversations with management. It is structured around eight key elements of effective AI governance frameworks.

**ELEMENT 1: Roles & responsibilities**
**ELEMENT 2: Governance structures**
**ELEMENT 3: People, skills & culture**
**ELEMENT 4: Principles, policies & strategy**
**ELEMENT 5: Practices, processes & controls**
**ELEMENT 6: Supporting infrastructure**
**ELEMENT 7: Stakeholder engagement & impact assessment**
**ELEMENT 8: Monitoring, reporting & evaluation**

[^2]: See HTI’s AI Governance Snapshot #1 Essential Components of AI Governance (HTI, 2024) for further information.

FOR PRACTICAL ASSISTANCE FOR DIRECTORS, THIS CHAPTER CONTAINS:

1. Key questions for directors to ask themselves and/or management; and
2. A traffic light system which assists directors process management’s response to key questions:

**AMBER** suggests there may be some risk, and advises that directors should probe further and assess management’s position and response. An uplift in governance practice may be necessary.

**RED** suggests there is potential high risk, and that directors should work with management to address this risk through implementing safe and responsible AI governance practices (as suggested in this guide).

The elements and questions featured in this section may also apply to the governance of some non-AI systems, or technology more broadly. This is deliberate – in ensuring that their organisations are adequately prepared to grasp the benefits and manage the risks of AI systems, directors can and should leverage existing governance knowledge and systems.

However, it is crucial that this knowledge and existing approaches are appropriately applied to the peculiar risks and concerns that the specific characteristics of AI systems create (see section 1.4 and Chapter 2 of *A Director’s Introduction to AI*).

Wherever possible, in each subsection we have highlighted where directors should look to the governance issues specific to AI.

This chapter is not intended to be a comprehensive guide. Regulatory requirements, guidance, and best practices in this area are rapidly evolving. Rather, directors should view these components as an ongoing conversation with management as AI governance continues to evolve.

### 2.1 Roles & responsibilities
[GO TO CONTENTS](#table-of-contents)

HTI’s research suggests that there is little awareness amongst corporate leaders of where, how and why AI systems are being used across their businesses.[^3] This lack of internal knowledge is a major barrier to AI governance efforts and amplifies AI risks.

Directors should be clear on which individual or body, at both the board and management level, has decision-making power and accountability for AI use.

While management will be responsible for AI implementation, the board has overall oversight over the organisation’s AI governance.

In the absence of a structured approach to AI system accountability, most organisations adopt a form of ‘guru-based governance’, where responsibility sits with a single individual viewed as technically competent in AI. Such over-reliance on a single leader or a small set of technical personnel within the organisation is problematic, not least because it creates significant key person risk.

**SUGGESTED DIRECTOR STEPS**

1. Determine and document which individual/body at the board and management level has responsibility, and is ultimately accountable to the board, for decisions regarding AI use. This includes a consideration of how to leverage existing governance structures (such as board and management committees) – see section 2.2.
2. Identify who is currently involved in, and accountable to the board for, decisions relating to the procurement, development and use of AI systems.
3. Determine and record where in the organisation AI is already being used. This could be in existing technology products. An AI inventory (Box 4) can provide a useful record of where AI is used within the organisation.
4. Consider whether decision-making processes applied by key accountable persons incorporate consideration of AI risk and opportunity.

**BOX 4: What is an AI inventory?**

An AI inventory or register is a structured, centralised, and up-to-date database of all AI systems that an organisation relies on, including those offered by third-party providers. The inventory should include details on the technical aspects of each system including:

- type of model and technology infrastructure being leveraged;
- the data used in both training and operation;
- the purpose and context of use;
- ongoing cost to the business; and
- the result of all recent risk and impact assessments.

An AI inventory is an essential asset for AI governance as it provides greater visibility into the mix of AI system types – their benefits, costs, and criticality, and the distribution of risk across systems and to stakeholders arising from AI use.

An AI inventory may also mitigate the risk of making incorrect or exaggerated claims about a product, service or company’s AI use (known as ‘AI-washing’).

[^3]: Lauren Solomon and Nicholas Davis, The State of AI Governance in Australia (HTI Report, 2023), 13.

**KEY QUESTIONS FOR DIRECTORS TO ASK**

- How are we tracking AI use within the organisation?
- Which individual or body at the board or management level is responsible for data governance?
- Which individual or body at the board or management level is responsible for decisions regarding the development and use of AI within the organisation?
- Which individual or body is responsible for making procurement decisions and identifying, assessing and reporting the risks associated with procurement? Are they tracking which procured products and services use AI?
- Is there an escalation protocol in place for proposed higher-risk AI uses?

**AMBER**

- Accountability for AI systems rests entirely with technical teams and/or relatively junior levels of management.
- Management is aware of internal AI system use, but has not assessed or documented employee or contractor use of third-party systems.
- Limited guidance/policy on use of AI and appropriate guardrails.
- Risk management frameworks are applied, but are not tailored to the amplified and new risks associated with AI.

**RED**

- AI understanding is highly concentrated in a few personnel.
- Management cannot confirm where and why AI systems are being used across the organisation.
- It is unclear who is responsible for the procurement, management, and outcomes of