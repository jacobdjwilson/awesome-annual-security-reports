# State of AI in Cybersecurity Report 2025

Ponemon Institute© Research Report

Sponsored by MixMode

MixMode.ai

1 (858) 225-2352 info@mixmode.ai © MixMode, Inc.

## Table of Contents
- [Forward](#forward)
- [Contents](#contents)
- [Part 1. Introduction](#part-1-introduction)
- [Part 2. Key Findings](#part-2-key-findings)
  - [Is AI making a difference?](#is-ai-making-a-difference)
  - [AI-powered cybersecurity tools](#ai-powered-cybersecurity-tools)
  - [Privacy, security and ethical considerations](#privacy-security-and-ethical-considerations)
- [Part 3. Methodology](#part-3-methodology)
- [Part 4. Caveats to this study](#part-4-caveats-to-this-study)

## Forward

The overall threat from China and other adversaries has only increased over time and has accelerated and been exacerbated with technological innovation and their access to AI. In an article from January 2025, Jen Easterly, former Director of the Cybersecurity and Infrastructure Security Agency (CISA), lays out some of the risks to US critical infrastructure. CISA defines critical infrastructure as encompassing 16 sectors from utilities to government agencies to banks and the entire IT industry.

Outages happen consistently across all sectors and vulnerabilities are everywhere. So, the key for all Cyber programs is continuing to improve upon early detection and early response.

After the Crowdstrike outage in 2024 that affected thousands of hospitals, airports and businesses worldwide, Easterly said, “We are building resilience into our networks and our systems so that we can withstand a significant disruption or at least drive down the recovery time to be able to provide services, which is why I thought the CrowdStrike incident — which was a terrible incident — was a useful exercise, like a dress rehearsal, for what China may want to do to us in some way and how we react if something like that happens,” she said. “We have to be able to respond very rapidly and recover very rapidly in a world where [an issue] is not reversible.” -https://therecord.media/easterly-china-cyberattacks-crowdstrike-outages.

What will organizations do to combat persistent threats and cyberattacks from increasingly sophisticated adversaries? A goal of this research MixMode sponsored is to provide information on how industry can leverage AI in their cybersecurity plans to detect attacks earlier (be predictive) and improve their ability to recover from attacks more quickly.

The MixMode Team

2 | MixMode.ai

1 (858) 225-2352 info@mixmode.ai © MixMode, Inc.

## Contents

Part 1. Introduction

Part 2. Key Findings

  A. Is AI making a difference?

  B. AI-powered cybersecurity tools

  C. Privacy, security and ethical considerations

Part 3. Methodology

Part 4. Caveats to this study

Publication Date: April 2025

4 | MixMode.ai

1 (858) 225-2352 info@mixmode.ai © MixMode, Inc.

Page 3

State of AI in Cybersecurity Report 2025

## Part 1. Introduction

Organizations are in a race to adopt artificial intelligence (AI) technologies to strengthen their ability to stop the constant threats from cyber criminals. This is the second annual study sponsored by MixMode on this topic. The purpose of this research is to understand since 2024 how organizations are leveraging AI to effectively detect and respond to cyberattacks.

Ponemon Institute surveyed 685 US IT and IT security practitioners in organizations that have adopted AI in some form. These respondents are familiar with their organization’s use of AI for cybersecurity and have responsibility for evaluating and/or selecting AI-based cybersecurity tools and vendors.

Since last year’s study, organizations have not made progress in their ability to integrate AI security technologies with legacy systems and streamline their security architecture to increase AI’s value. According to Figure 1, more respondents believe it is difficult to integrate AI- based security technologies with legacy systems, an increase from 65 percent to 70 percent of respondents. Sixty-seven percent of respondents, a slight increase from 64 percent of respondents, say their organizations need to simplify and streamline its security architecture to obtain maximum value from AI. Most organizations continue to use AI to detect attacks across the cloud, on-premises and hybrid environments.

![Figure 1. Trends in AI adoption - A bar chart showing trends in AI adoption from FY2024 to FY2025. The chart displays three metrics: "It is difficult to integrate AI-based security technologies with legacy systems," "Our organization needs to simplify and streamline its security architecture to obtain maximum value from AI-based security technologies," and "Our organization uses AI to detect attacks across cloud, on-premises and hybrid environments." For the first metric, percentages increased from 65% to 70%. For the second metric, percentages increased from 64% to 67%. For the third metric, percentages increased from 66% to 72% over the years.](Image description)

The following research findings reveal the benefits and challenges of AI.

How organizations are using AI to improve their security posture.

In just one year since the research was first conducted, organizations are reporting that their security posture has significantly improved because of AI. The biggest changes are improving the ability to prioritize threats and vulnerabilities (an increase from 50 percent to 56 percent of respondents), increasing the efficiency of the SOC team (from 43 percent to 51 percent) and increasing the speed of analyzing threats (from 36 percent to 43 percent).

Since 2024, the maturity of AI programs has increased. Fifty-three percent of organizations have achieved full adoption stage (31 percent of respondents) or mature stage (22 percent of respondents). This is an increase from 2024 when 47 percent respondents said they had reached the full adoption stage (29 percent of respondents) or mature stage (18 percent of respondents).

AI-based security technologies increase productivity and job satisfaction. Seventy percent of respondents say AI increases productivity of IT security personnel, an increase from 66 percent in 2024. Fifty-one percent of respondents say AI improves the efficiency of junior analysts so that senior analysts can focus on critical threats and strategic projects. Sixty-nine percent of respondents say since the adoption of AI, job satisfaction has improved because of the elimination of tedious tasks, an increase from 64 percent.

Forty-four percent of respondents are using AI-powered cybersecurity tools or solutions. By leveraging advanced algorithms and machine learning techniques. AI-powered systems analyze vast amounts of data, identify patterns and adapt their behavior to improve performance over time.

Forty-three percent of respondents are using pre-emptive security tools to stay ahead of cyber criminals. Pre-emptive security tools apply AI-based data analysis to cybersecurity so organizations can anticipate and prevent future attacks. The benefits include the ability to preemptively deter threats and minimize damages, prioritize tasks effectively and address the most important business risks first. Pre-emptive security data can guide response teams, offer insights into the attack’s objectives, potential targets and more. The result is continuous improvement to ensure more accurate forecasts and reduce costs associated with handling attacks

Respondents say pre-emptive security is used to identify patterns that signal impending threats (60 percent), assess risks to identify emerging threats and potential impact (57 percent) and is used to harness vast amounts of online metadata from various sources as an input to predictive analytics (52 percent).

Pre-emptive security will decrease the ability of cybercriminals to direct targeted attacks. Fifty-two percent of respondents in organizations that use pre-emptive security say that without it cybercriminals will become more successful at directing targeted attacks at unprecedented speed and scale while going undetected by traditional, rule-based detection. Forty-nine percent say investments are being made in pre-emptive AI to stop AI-driven cybercrimes.

Fifty-eight percent of respondents say their SOCs use AI technologies. The primary benefit of an AI-powered SOC is that alerts are resolved faster, according to 57 percent of respondents. In addition to faster resolution of alerts, 55 percent of respondents say it frees up analyst bandwidth to focus on urgent incidents and strategic projects. Fifty percent of respondents say it applies real-time intelligence to identify patterns and detect emerging threats.

An AI-powered SOC is effective in reducing threats. Human analysts are effective as the final line of defense in the AI-powered SOC. Fifty-seven percent of respondents say AI in the SOC is very or highly effective in reducing threats and 50 percent of respondents say their human analysts are very or highly effective as the final line of defense in the AI-powered SOC.

More organizations are creating one unified approach to managing both AI and privacy security risks, an increase from 37 percent to 52 percent of respondents. In addition, 58 percent of respondents say their organizations identify vulnerabilities and what can be done to eliminate them.

The barriers and challenges to maximizing the value from AI

While an insufficient budget to invest in AI technologies continues to be the primary governance challenge, more organizations say an increase in internal expertise is needed to validate vendors’ claims. The lack of internal expertise to validate vendors’ claims increased significantly from 53 percent to 59 percent of respondents. One of the key takeaways from the research is that 63 percent of respondents say the decision to invest in AI technologies is based on the extensiveness of the vendors’ expertise.

As the number of cyberattacks increase, especially malicious insider incidents, organizations lack confidence in their ability to prevent risks and threats. Fifty-one percent of respondents say their organizations had at least one cyberattack in the past 12 months, an increase from 45 percent of respondents in 2024.

Only 42 percent say their organizations are very or highly effective in mitigating risks, vulnerabilities and attacks across the enterprise. The attacks that increased since 2024 are malicious insiders (53 percent vs. 45 percent), compromised/stolen devices (40 percent vs. 35 percent) and credential theft (49 percent vs. 53 percent). The primary types of attacks in 2024 and 2025 are phishing/social engineering and web-based attacks.

The effectiveness of AI technologies is diminished because of interoperability issues and an increase in a heavy reliance on legacy IT environments. The barriers to AI-based security technologies’ effectiveness are interoperability issues (63 percent, an increase from 60 percent of respondents), can’t apply AI-based controls that span across the entire enterprise (59 percent vs. 61 percent of respondents) and can’t create a unified view of AI users across the enterprise (56 percent vs 58 percent of respondents). The most significant trend is the increase in the heavy reliance on legacy IT environments, an increase from 36 percent to 45 percent of respondents.

Complexity challenges the preparedness of cybersecurity teams to work with AI-powered tools. Only 42 percent of respondents say their cybersecurity teams are highly prepared to work with AI-powered tools. Fifty-five percent of respondents say AI-powered solutions are highly complex.

AI continues to make it difficult to comply with privacy and security mandates and to safeguard confidential and personal data in AI. Forty-eight percent of respondents say it is highly difficult to achieve compliance and 53 percent of respondents say it is highly difficult to safeguard confidential and personal data in AI

6 | MixMode.ai

1 (858) 225-2352 info@mixmode.ai © MixMode, Inc.

Page 4

State of AI in Cybersecurity Report 2025

## Part 2. Key Findings

In this section, we provide a deeper dive into the research. The complete research findings are presented in the Appendix. The report is organized according to the following topics.

§ Is AI making a difference?
§ AI-powered cybersecurity tools
§ Getting ahead of cyber criminals with pre-emptive AI solutions
§ The use of AI in the SOC
§ Privacy, security and ethical considerations

### Is AI making a difference?

While an insufficient budget to invest in AI technologies continues to be the primary governance challenge, more organizations say an increase in internal expertise is needed to validate vendors’ claims. According to Figure 2, 59 percent of respondents say an insufficient budget to invest in AI-based technologies continues to be a challenge, an increase from 56 percent. The lack of internal expertise to validate vendors’ claims increased significantly from 53 percent to 59 percent of respondents. This challenge can affect investments in AI technologies because 63 percent of respondents say the decision to invest in AI is based on a review of a vendor’s extensive expertise.

![Figure 2. Organizational or governance challenges - A bar chart showing organizational or governance challenges in FY2024 and FY2025. The chart displays several challenges, including "There is insufficient budget for AI-based technologies," "We don’t have the internal expertise to validate vendors’ claims," "There is not enough time to integrate AI-based technologies into security workflows," "It requires too much staff to implement and maintain AI-based technologies," "IT and IT security functions are not aligned on the organization’s AI strategy," "We can’t recruit personnel experienced in AI-based technologies," "There is insufficient supervision and oversight of AI learning and decision-making," and "Other." For "There is insufficient budget for AI-based technologies," percentages increased from 56% to 59%. For "We don’t have the internal expertise to validate vendors’ claims," percentages increased from 53% to 59%. For "There is not enough time to integrate AI-based technologies into security workflows," percentages decreased from 42% to 45%. For "It requires too much staff to implement and maintain AI-based technologies," percentages decreased from 39% to 36%. For "IT and IT security functions are not aligned on the organization’s AI strategy," percentages decreased from 37% to 35%. For "We can’t recruit personnel experienced in AI-based technologies," percentages decreased from 36% to 33%. For "There is insufficient supervision and oversight of AI learning and decision-making," percentages decreased from 29% to 27%. For "Other," percentages decreased from 8% to 6%.](Image description)

Malicious insider attacks increased significantly since 2024. Fifty-one percent of respondents had at least one cyberattack in the past 12 months, an increase from 2024 of 45 percent of respondents. Accordingly, when asked to rate the effectiveness of their IT security posture in terms of its effectiveness at mitigating risks, vulnerabilities and attacks across the enterprise on a scale of 1 = not effective to 10 = highly effective, only 42 percent say their organizations are very or highly effective (7+ on the 10-point scale).

As shown in Figure 3, the attacks that increased are malicious insiders (53 percent vs. 45 percent), compromised/stolen devices (40 percent vs. 35 percent) and credential theft (49 percent vs. 53 percent). Phishing/social engineering continues to be a primary security threat.

![Figure 3. What best describes the type of attacks experienced by your organization? - A bar chart showing the types of attacks experienced by organizations in FY2024 and FY2025. The chart lists various attack types, including "Phishing / social engineering," "Malicious insider," "Credential theft," "Web-based attack," "Denial of service," "Cloud malware injection attacks," "Advanced malware," "Compromised / stolen devices," "Web application attack," "Ransomware," "Zero-day attacks *," "Cloud crypto mining," "APIs," "AI-generated attacks," "Account takeover," and "Other." For "Phishing / social engineering," percentages were 56% in FY2024 and 53% in FY2025. For "Malicious insider," percentages increased from 45% to 53%. For "Credential theft," percentages increased from 49% to 53%. For "Web-based attack," percentages were 51% in FY2024 and 48% in FY2025. For "Denial of service," percentages were 45% in FY2024 and 48% in FY2025. For "Cloud malware injection attacks," percentages were 42% in FY2024 and 41% in FY2025. For "Advanced malware," percentages were 35% in FY2024 and 40% in FY2025. For "Compromised / stolen devices," percentages increased from 35% to 40%. For "Web application attack," percentages were 33% in FY2024 and 37% in FY2025. For "Ransomware," percentages were 31% in FY2024 and 37% in FY2025. For "Zero-day attacks *," percentages were 29% in FY2024 and 26% in FY2025. For "Cloud crypto mining," percentages were 29% in FY2024 and 25% in FY2025. For "APIs," percentages were 23% in FY2024 and 29% in FY2025. For "AI-generated attacks," percentages were 15% in FY2024 and 23% in FY2025. For "Account takeover," percentages were 10% in FY2024 and 17% in FY2025. For "Other," percentages were 5% in FY2024 and 10% in FY2025.](Image description)

A positive finding is that despite the challenges in adoption, more organizations in the 2025 research believe AI is making a difference in their security posture. According to Figure 4, AI improves organizations’ security posture by improving the ability to prioritize threats and vulnerabilities (56 percent, an increase from 50 percent), the efficiency of the SOC team (51 percent, an increase from 43 percent) and the speed of analyzing threats (43 percent, an increase from 36 percent).

![Figure 4. How does AI improve your organization’s security posture? - A bar chart showing how AI improves an organization's security posture in FY2024 and FY2025. The chart lists several improvements, including "Improves the ability to prioritize threats and vulnerabilities," "Improves the efficiency of the SOC team," "Increases the speed of analyzing threats," "Reduces the number of security events that must be investigated," "Reduces the mean time to detect and respond to security incidents," "Reduces the false positive and/or false negative rates," "Reduces the manual updating of firewall rules and security policies," "Provides more in-depth knowledge about security threats," "Provides security intelligence about network traffic and entities*," "Reduces the complexity of the cyber security architecture," and "Other*." For "Improves the ability to prioritize threats and vulnerabilities," percentages increased from 50% to 56%. For "Improves the efficiency of the SOC team," percentages increased from 43% to 51%. For "Increases the speed of analyzing threats," percentages increased from 36% to 43%. For "Reduces the number of security events that must be investigated," percentages remained at 40%. For "Reduces the mean time to detect and respond to security incidents," percentages remained at 35%. For "Reduces the false positive and/or false negative rates," percentages increased from 27% to 28%. For "Reduces the manual updating of firewall rules and security policies," percentages increased from 27% to 28%. For "Provides more in-depth knowledge about security threats," percentages increased from 23% to 33%. For "Provides security intelligence about network traffic and entities*," percentages increased from 23% to 27%. For "Reduces the complexity of the cyber security architecture," percentages remained at 23%. For "Other*," percentages remained at 23%.](Image description)

The effectiveness of AI technologies is diminished because of interoperability issues and an increase in a heavy reliance on legacy IT environments. According to Figure 5, the barriers to AI-based security technologies’ effectiveness are interoperability issues (63 percent, an increase from 60 percent), can’t apply AI-based controls that span across the entire enterprise (59 percent vs. 61 percent) and can’t create a unified view of AI users across the enterprise (56 percent vs 58 percent).

The most significant trend is the increase in the heavy reliance on legacy IT environments, an increase from 36 percent to 45 percent. As discussed in this report, 70 percent of respondents say it is difficult to integrate AI- based security technologies with legacy systems.

![Figure 5. Barriers to the effectiveness of AI-based security technologies used today - A bar chart showing barriers to the effectiveness of AI-based security technologies in FY2024 and FY2025. The chart lists several barriers, including "There are Interoperability issues among AI technologies," "We can’t apply AI-based controls that span across the entire enterprise," "We can’t create a unified view of AI users across the enterprise," "There are errors and inaccuracies in AI decision rules," "AI tools/technology we need are not available," "There is a heavy reliance on legacy IT environments," "There are errors and inaccuracies in data inputs ingested by AI technology (engine)," and "There is a lack of mature and/or stable AI technologies." For "There are Interoperability issues among AI technologies," percentages increased from 60% to 63%. For "We can’t apply AI-based controls that span across the entire enterprise," percentages decreased from 61% to 59%. For "We can’t create a unified view of AI users across the enterprise," percentages decreased from 58% to 56%. For "There are errors and inaccuracies in AI decision rules," percentages increased from 51% to 53%. For "AI tools/technology we need are not available," percentages decreased from 48% to 46%. For "There is a heavy reliance on legacy IT environments," percentages increased from 36% to 45%. For "There are errors and inaccuracies in data inputs ingested by AI technology (engine)," percentages increased from 45% to 48%. For "There is a lack of mature and/or stable AI technologies," percentages increased from 28% to 32%. For "Other," percentages increased from 5% to 6%.](Image description)

When investing in AI technologies, most organizations consider the expertise of vendors. As shown in Figure 6, 63 percent of respondents say extensive expertise is important followed by the vendor’s ability to combine sophisticated software with a practiced methodology for implementing AI (45 percent of respondents). However, as shown in the research 56 percent of respondents say their organizations do not have the necessary internal expertise to validate vendors’ claims.

The average IT security budget for 2025 is $36.8 million and an average of 21 percent or $7.9 million is allocated to AI/ML investments.

![Figure 6. Which of the following factors are most important when investing in AI security technologies? - A bar chart showing factors important when investing in AI security technologies. The chart lists several factors, including "How extensive is the vendor’s expertise," "The vendor should combine sophisticated software with a practiced methodology for implementing AI," "Identify the problem or question your organization is trying to solve and what would be the ROI," and "The techniques vendors use to build machine learning models and develop AI systems." For "How extensive is the vendor’s expertise," 63% of respondents selected this. For "The vendor should combine sophisticated software with a practiced methodology for implementing AI," 45% of respondents selected this. For "Identify the problem or question your organization is trying to solve and what would be the ROI," 37% of respondents selected this. For "The techniques vendors use to build machine learning models and develop AI systems," 33% of respondents selected this.](Image description)

Since 2024, the maturity of organizations’ use of AI has increased. As shown in Figure 7 2025, 53 percent of organizations have achieved full adoption stage (31 percent) or mature stage (22 percent), as described below. This is an increase from 2024 when 47 percent say they have reached the full adoption stage (29 percent) or mature stage (18 percent).

![Figure 7. What best describes the maturity of your organization’s use of AI? - A bar chart showing the maturity of AI use in organizations in FY2024 and FY2025. The chart displays three maturity stages: "Mature stage – AI in cybersecurity activities are fully deployed and security risks assessed. Effectiveness of AI is measured with KPIs and C-level executives are regularly informed about AI’s ability to prevent and reduce cyberattacks," "Full adoption stage – AI technologies are mostly deployed. The program has C-level support and adequate budget," and "Early adoption stage – We have defined what the organization’s cybersecurity AI strategy is, investments are planned and partially deployed." For "Mature stage," percentages increased from 18% to 22%. For "Full adoption stage," percentages increased from 29% to 31%. The combined percentage for "Mature stage" and "Full adoption stage" increased from 47% to 53%.](Image description)

AI-based security technologies improve productivity and job satisfaction. According to Figure 8, 70 percent of respondents say AI increases productivity of IT security personnel, an increase from 66 percent in 2024. Fifty-one percent of respondents say AI improves the efficiency of junior analysts so that senior analysts can focus on critical threats and strategic projects. Sixty-nine percent of respondents say since the adoption of AI job satisfaction has improved because of the elimination of tedious tasks, an increase from 64 percent.

![Figure 8. The impact of AI on productivity and job satisfaction - A bar chart showing the impact of AI on productivity and job satisfaction in FY2024 and FY2025. The chart lists several impacts, including "The deployment of AI-based security technologies will increase the productivity of IT security personnel," "Since the adoption of AI, job satisfaction has improved because of the elimination of tedious tasks," "Our organization is investing in AI because of the shortage of cybersecurity expertise," and "AI improves the efficiency of junior analysts so that senior analysts can focus on critical threats and strategic projects*." For "The deployment of AI-based security technologies will increase the productivity of IT security personnel," percentages increased from 66% to 70%. For "Since the adoption of AI, job satisfaction has improved because of the elimination of tedious tasks," percentages increased from 64% to 69%. For "Our organization is investing in AI because of the shortage of cybersecurity expertise," percentages increased from 50% to 52%. For "AI improves the efficiency of junior analysts so that senior analysts can focus on critical threats and strategic projects*," 51% of respondents selected this in FY2025.](Image description)

The need to increase in-house expertise and dedicated headcount has declined significantly. However, outside expertise is needed to maximize the value of AI-based technologies. According to Figure 9, in 2024 65 percent of respondents said they would need more in-house expertise and dedicated headcount because of AI-based security technologies. In 2025, 59 percent of respondents say more staffing is needed. The decline could be attributed to evidence in the research that AI improves productivity. Fifty-six percent of respondents say their organizations need outside expertise to maximize the value of AI-based security technologies, a slight increase from 54 percent of respondents.

![Figure 9. The impact of AI on staffing - A bar chart showing the impact of AI on staffing in FY2024 and FY2025. The chart lists several impacts, including "AI-based security technologies will increase our organization’s need for in-house expertise and dedicated headcount," "Based on the security capabilities currently in place, our organization is not adequately prepared to defend against AI-powered threats and attacks*," and "Our organization needs outside expertise to maximize the value of AI-based security technologies." For "AI-based security technologies will increase our organization’s need for in-house expertise and dedicated headcount," percentages decreased from 65% to 59%. For "Based on the security capabilities currently in place, our organization is not adequately prepared to defend against AI-powered threats and attacks*," 56% of respondents selected this in FY2025. For "Our organization needs outside expertise to maximize the value of AI-based security technologies," percentages increased from 54% to 56%.](Image description)

To determine AI’s effectiveness, more organizations are measuring the SOC team’s increased ability to detect and respond to threats, an increase from 52 percent of respondents to 61 percent of respondents. As shown in Figure 10, an important measure is the cost of cybersecurity operations. However, the use of this measure decreased from 63 percent to 53 percent of respondents. The prevention of security incidents as a measure increased significantly from 40 percent to 50 percent of respondents.

![Figure 10. How does your organization determine the effectiveness of AI? - A bar chart showing how organizations determine the effectiveness of AI in FY2024 and FY2025. The chart lists several methods, including "Increases the SOC team’s ability to detect and respond to threats," "Decreases cost of cybersecurity operations," "Security incidents prevented," "Generates Return on Investment," "Increases ability to meet compliance mandates," and "Reduces Total Cost of Ownership." For "Increases the SOC team’s ability to detect and respond to threats," percentages increased from 52% to 61%. For "Decreases cost of cybersecurity operations," percentages decreased from 63% to 53%. For "Security incidents prevented," percentages increased from 40% to 50%. For "Generates Return on Investment," percentages increased from 49% to 48%. For "Increases ability to meet compliance mandates," percentages increased from 39% to 47%. For "Reduces Total Cost of Ownership," percentages increased from 37% to 40%. For "Other," percentages increased from 3% to 5%.](Image description)

In AI there are a variety of technologies and respondents were asked if they are very familiar or familiar with them as defined below.

Unsupervised learning in artificial intelligence is a type of machine learning that learns from data without human supervision. Unlike supervised learning, unsupervised machine learning models are given unlabeled data and allowed to discover patterns and insights without any explicit guidance or instruction.

Supervised machine learning is a machine learning technique that uses labeled data to train algorithms to predict outcomes. The goal is to create a model that can accurately predict outputs on new data.

Generative AI produces new content, such as text, images or music applications. Machine learning is used for tasks like recommendation systems, predictive analytics and diagnostic tools

Self-supervised learning (SSL) is a machine learning technique that trains models to learn from unlabeled data. It’s a middle ground between supervised and unsupervised learning.

Natural language processing (NLP) is a technology that enables computers to understand human language. It’s a key part of AI and is used in many applications, such as Chatbots and machine translation.

Generative adversarial networks (GANs) are machine learning models that create new data that resembles real data. GANs are made up of two neural networks that compete against each other.

Deep learning models can recognize data patterns like complex pictures, text, and sounds to produce accurate insights and predictions. A neural network is the underlying technology in deep learning. It consists of interconnected nodes or neurons in a layered structure.

15 | MixMode.ai

1 (858) 225-2352 info@mixmode.ai © MixMode, Inc.

Page 14

State of AI in Cybersecurity Report 2025

As shown in Figure 11, respondents are most familiar with unsupervised machine learning and generative AI/large language models, 60 percent and 59 percent of respondents, respectively.

![Figure 11. Familiarity with AI-based technologies - A bar chart showing familiarity with AI-based technologies. The chart lists several AI technologies, including "Familiarity with unsupervised machine learning (trained on unlabeled data)," "Familiarity with Generative AI/large language models," "Familiarity with supervised machine learning (trained on selected data sets)," "Familiarity with self-supervised machine learning (learns and adapts on its own)," "Familiarity with Natural Language Processing," "Familiarity with Generative Adversarial Networks," and "Familiarity with Deep Learning and Neural Networks." For "Familiarity with unsupervised machine learning (trained on unlabeled data)," 60% of respondents are familiar. For "Familiarity with Generative AI/large language models," 59% of respondents are familiar. For "Familiarity with supervised machine learning (trained on selected data sets)," 56% of respondents are familiar. For "Familiarity with self-supervised machine learning (learns and adapts on its own)," 49% of respondents are familiar. For "Familiarity with Natural Language Processing," 41% of respondents are familiar. For "Familiarity with Generative Adversarial Networks," 38% of respondents are familiar. For "Familiarity with Deep Learning and Neural Networks," 38% of respondents are familiar.](Image description)

### AI-powered cybersecurity tools

AI-powered cybersecurity tools or solutions incorporate AI to perform tasks or make decisions typically requiring human intelligence. By leveraging advanced algorithms and machine learning techniques. AI-powered systems analyze vast amounts of data, identify patterns and adapt their behavior to improve performance over time.

Forty-four percent of respondents say their organizations use AI-powered tools or solutions. Thirty-six percent of these respondents say their organizations have fully deployed and 64 percent of respondents say these tools are partially deployed. Figure 12 lists the AI-powered tools or solutions their organizations use or plan to use.

Extended Detection and Response (XDR) is a cybersecurity technology that monitors and responds to threats. XDR is a unified security platform that combines data from various security tools into a single console and is used by 44 percent of organizations. Endpoint Detection and Response (EDR) is a cybersecurity technology that monitors devices for cyber threats. EDR can help prevent cyber criminals for using devices to access data and networks and is used by 40 percent of organizations.

Network Detection and Response (NDR) is a cybersecurity technology that monitors network traffic for threats, uses machine learning and analytics to identify suspicious activity can be delivered through hardware, software or SaaS and is used by 38 percent of organizations.

![Figure 12. What AI-powered cybersecurity tools or solutions does your organization use or plan to use? - A bar chart showing AI-powered cybersecurity tools or solutions used or planned to be used by organizations. The chart lists several tools, including "XDR," "EDR," "NDR," "Cloud Detection and Response," "SIEM," and "Other." For "XDR," 44% of organizations use or plan to use it. For "EDR," 40% of organizations use or plan to use it. For "NDR," 38% of organizations use or plan to use it. For "Cloud Detection and Response," 37% of organizations use or plan to use it. For "SIEM," 37% of organizations use or plan to use it. For "Other," 6% of organizations use or plan to use it.](Image description)

Figure 13 lists the other AI-powered tools organizations use or plan to use. As shown, Looka and ChatGPT are the most frequently used according to 36 percent and 34 percent of respondents, respectively.

Following are descriptions of AI-powered tools or solutions used. Looka’s AI powered platform is used to design logos. ChatGPT is a generative AI Chatbot developed by open AI. Lumen5 is a video creation platform powered by AI. Fireflies transcribes, summarizes and analyzes team conversations. Krisp is AI-based audio processing software that offers real-time noise and voice suppression technology. Stable Diffusion is a generative AI model that produces unique photo realistic images from text and images. Dall-E2 is an AI system that can create realistic images and text for a desig. Legal Robot provides automate analysis of legal documents and linguistic/statistical analysis to help understand potential issues.

![Figure 13. What other AI-powered tools or solutions does your organization use or plan to use? - A bar chart showing other AI-powered tools or solutions used or planned to be used by organizations. The chart lists several tools, including "Looka," "ChatGPT," "Lumen5," "Fireflies," "Krisp," "Stable Diffusion," "Dall-E2," and "Legal Robot." For "Looka," 36% of organizations use or plan to use it. For "ChatGPT," 34% of organizations use or plan to use it. For "Lumen5," 27% of organizations use or plan to use it. For "Fireflies," 24% of organizations use or plan to use it. For "Krisp," 23% of